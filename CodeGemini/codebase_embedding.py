#!/usr/bin/env python3
"""
CodeGemini Codebase Embedding Module
ç¨‹å¼ç¢¼åº« Embedding æ¨¡çµ„ - æä¾›æ·±åº¦ç¨‹å¼ç¢¼ç†è§£åŠŸèƒ½

æ­¤æ¨¡çµ„è² è²¬ï¼š
1. å°æ•´å€‹ codebase å»ºç«‹ embedding
2. æœå°‹ç›¸ä¼¼ç¨‹å¼ç¢¼
3. ç²å–æª”æ¡ˆä¸Šä¸‹æ–‡
4. æ·±åº¦ç†è§£ç¨‹å¼ç¢¼çµæ§‹

æŠ€è¡“æ£§ï¼š
- Gemini Text Embedding APIï¼ˆå…è²»ï¼‰
- SQLite + NumPyï¼ˆè¼•é‡ç´šå‘é‡è³‡æ–™åº«ï¼‰
- æ”¯æ´ Python 3.14+
"""

import logging
import sqlite3
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import hashlib

# è¨­ç½® logger
logger = logging.getLogger(__name__)


@dataclass
class CodeChunk:
    """ç¨‹å¼ç¢¼åˆ†å¡Šè³‡æ–™çµæ§‹"""
    file_path: str
    chunk_id: str
    content: str
    chunk_type: str  # 'function', 'class', 'file', 'conversation'
    start_line: int
    end_line: int
    language: str
    embedding: Optional[List[float]] = None
    metadata: Optional[Dict[str, Any]] = None  # é¡å¤–çš„å…ƒæ•¸æ“šï¼ˆå¦‚å°è©±æ™‚é–“ã€ç”¨æˆ¶ ID ç­‰ï¼‰

    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return asdict(self)


class VectorDatabase:
    """è¼•é‡ç´šå‘é‡è³‡æ–™åº«ï¼ˆåŸºæ–¼ SQLite + NumPyï¼‰

    æ›¿ä»£ ChromaDBï¼Œæ”¯æ´ Python 3.14+
    """

    def __init__(self, db_path: str = ".embeddings/vector.db"):
        """åˆå§‹åŒ–å‘é‡è³‡æ–™åº«

        Args:
            db_path: è³‡æ–™åº«æª”æ¡ˆè·¯å¾‘
        """
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        self.conn = sqlite3.connect(str(self.db_path))
        self._create_tables()
        logger.info(f"âœ“ VectorDatabase å·²åˆå§‹åŒ–: {self.db_path}")

    def _create_tables(self):
        """å»ºç«‹è³‡æ–™è¡¨"""
        cursor = self.conn.cursor()

        # å„²å­˜ç¨‹å¼ç¢¼åˆ†å¡Šèˆ‡ embedding
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS code_chunks (
                chunk_id TEXT PRIMARY KEY,
                file_path TEXT NOT NULL,
                content TEXT NOT NULL,
                chunk_type TEXT NOT NULL,
                start_line INTEGER NOT NULL,
                end_line INTEGER NOT NULL,
                language TEXT NOT NULL,
                embedding BLOB,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # å»ºç«‹ç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_file_path
            ON code_chunks(file_path)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_chunk_type
            ON code_chunks(chunk_type)
        """)

        self.conn.commit()

    def add_chunk(self, chunk: CodeChunk):
        """æ–°å¢ç¨‹å¼ç¢¼åˆ†å¡Š

        Args:
            chunk: CodeChunk å¯¦ä¾‹
        """
        cursor = self.conn.cursor()

        # å°‡ embedding è½‰æ›ç‚º numpy array ä¸¦åºåˆ—åŒ–
        embedding_blob = None
        if chunk.embedding:
            embedding_blob = np.array(chunk.embedding, dtype=np.float32).tobytes()

        # å°‡ metadata åºåˆ—åŒ–ç‚º JSON
        metadata_json = None
        if chunk.metadata:
            metadata_json = json.dumps(chunk.metadata, ensure_ascii=False)

        cursor.execute("""
            INSERT OR REPLACE INTO code_chunks
            (chunk_id, file_path, content, chunk_type, start_line, end_line, language, embedding, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            chunk.chunk_id,
            chunk.file_path,
            chunk.content,
            chunk.chunk_type,
            chunk.start_line,
            chunk.end_line,
            chunk.language,
            embedding_blob,
            metadata_json
        ))

        self.conn.commit()

    def get_chunk(self, chunk_id: str) -> Optional[CodeChunk]:
        """ç²å–ç¨‹å¼ç¢¼åˆ†å¡Š

        Args:
            chunk_id: åˆ†å¡Š ID

        Returns:
            CodeChunk å¯¦ä¾‹æˆ– None
        """
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT chunk_id, file_path, content, chunk_type, start_line, end_line, language, embedding, metadata
            FROM code_chunks
            WHERE chunk_id = ?
        """, (chunk_id,))

        row = cursor.fetchone()
        if not row:
            return None

        # ååºåˆ—åŒ– embedding
        embedding = None
        if row[7]:
            embedding = np.frombuffer(row[7], dtype=np.float32).tolist()

        # ååºåˆ—åŒ– metadata
        metadata = None
        if row[8]:
            metadata = json.loads(row[8])

        return CodeChunk(
            chunk_id=row[0],
            file_path=row[1],
            content=row[2],
            chunk_type=row[3],
            start_line=row[4],
            end_line=row[5],
            language=row[6],
            embedding=embedding,
            metadata=metadata
        )

    def get_all_chunks(self) -> List[CodeChunk]:
        """ç²å–æ‰€æœ‰ç¨‹å¼ç¢¼åˆ†å¡Š

        Returns:
            CodeChunk åˆ—è¡¨
        """
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT chunk_id, file_path, content, chunk_type, start_line, end_line, language, embedding, metadata
            FROM code_chunks
        """)

        chunks = []
        for row in cursor.fetchall():
            embedding = None
            if row[7]:
                embedding = np.frombuffer(row[7], dtype=np.float32).tolist()

            metadata = None
            if row[8]:
                metadata = json.loads(row[8])

            chunks.append(CodeChunk(
                chunk_id=row[0],
                file_path=row[1],
                content=row[2],
                chunk_type=row[3],
                start_line=row[4],
                end_line=row[5],
                language=row[6],
                embedding=embedding,
                metadata=metadata
            ))

        return chunks

    def search_similar(self, query_embedding: List[float], top_k: int = 5) -> List[Tuple[CodeChunk, float]]:
        """æœå°‹ç›¸ä¼¼ç¨‹å¼ç¢¼åˆ†å¡Š

        Args:
            query_embedding: æŸ¥è©¢å‘é‡
            top_k: è¿”å›å‰ k å€‹çµæœ

        Returns:
            (CodeChunk, similarity_score) åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åº
        """
        all_chunks = self.get_all_chunks()

        if not all_chunks:
            return []

        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        query_vec = np.array(query_embedding, dtype=np.float32)
        query_norm = np.linalg.norm(query_vec)

        results = []
        for chunk in all_chunks:
            if not chunk.embedding:
                continue

            chunk_vec = np.array(chunk.embedding, dtype=np.float32)
            chunk_norm = np.linalg.norm(chunk_vec)

            # é¤˜å¼¦ç›¸ä¼¼åº¦
            if query_norm > 0 and chunk_norm > 0:
                similarity = np.dot(query_vec, chunk_vec) / (query_norm * chunk_norm)
                results.append((chunk, float(similarity)))

        # æ’åºä¸¦è¿”å›å‰ k å€‹
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    def delete_file_chunks(self, file_path: str):
        """åˆªé™¤æŒ‡å®šæª”æ¡ˆçš„æ‰€æœ‰åˆ†å¡Š

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
        """
        cursor = self.conn.cursor()
        cursor.execute("DELETE FROM code_chunks WHERE file_path = ?", (file_path,))
        self.conn.commit()

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        cursor = self.conn.cursor()

        cursor.execute("SELECT COUNT(*) FROM code_chunks")
        total_chunks = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(DISTINCT file_path) FROM code_chunks")
        total_files = cursor.fetchone()[0]

        cursor.execute("SELECT chunk_type, COUNT(*) FROM code_chunks GROUP BY chunk_type")
        chunk_type_counts = dict(cursor.fetchall())

        return {
            'total_chunks': total_chunks,
            'total_files': total_files,
            'chunk_type_counts': chunk_type_counts,
            'db_path': str(self.db_path),
            'db_size_mb': round(self.db_path.stat().st_size / 1024 / 1024, 2) if self.db_path.exists() else 0
        }

    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£æ¥"""
        self.conn.close()


class CodebaseEmbedding:
    """ç¨‹å¼ç¢¼åº« Embedding ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    - å°æ•´å€‹ codebase å»ºç«‹ embedding
    - æœå°‹ç›¸ä¼¼ç¨‹å¼ç¢¼
    - ç²å–æª”æ¡ˆä¸Šä¸‹æ–‡
    - æ·±åº¦ç†è§£ç¨‹å¼ç¢¼çµæ§‹

    ä½¿ç”¨ Gemini Text Embedding APIï¼ˆå…è²»ï¼‰
    """

    # æ”¯æ´çš„ç¨‹å¼èªè¨€å‰¯æª”å
    SUPPORTED_EXTENSIONS = {
        '.py': 'python',
        '.js': 'javascript',
        '.ts': 'typescript',
        '.jsx': 'jsx',
        '.tsx': 'tsx',
        '.go': 'go',
        '.java': 'java',
        '.cpp': 'cpp',
        '.c': 'c',
        '.rs': 'rust',
        '.rb': 'ruby',
        '.php': 'php',
        '.swift': 'swift',
        '.kt': 'kotlin',
        '.scala': 'scala',
        '.sh': 'bash',
        '.md': 'markdown'
    }

    def __init__(
        self,
        vector_db_path: str = ".embeddings",
        api_key: Optional[str] = None,
        collection_name: str = "codebase",
        orthogonal_mode: bool = False,
        similarity_threshold: float = 0.85
    ):
        """åˆå§‹åŒ– Codebase Embedding

        Args:
            vector_db_path: å‘é‡è³‡æ–™åº«è·¯å¾‘
            api_key: Gemini API Key
            collection_name: Collection åç¨±ï¼ˆä¿ç•™åƒæ•¸ï¼Œèˆ‡ ChromaDB å…¼å®¹ï¼‰
            orthogonal_mode: æ˜¯å¦å•Ÿç”¨æ­£äº¤æ¨¡å¼ï¼ˆä¿æŒå…§å®¹ç·šæ€§ç¨ç«‹ï¼‰
            similarity_threshold: æ­£äº¤æ¨¡å¼ä¸‹çš„ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆé è¨­ 0.85ï¼‰
        """
        try:
            import google.generativeai as genai
            self.genai = genai
            self.has_genai = True

            if api_key:
                genai.configure(api_key=api_key)
                logger.info("âœ“ Gemini API å·²é…ç½®")

        except ImportError:
            self.genai = None
            self.has_genai = False
            logger.warning("âœ— google-generativeai æœªå®‰è£ï¼ŒEmbedding åŠŸèƒ½å—é™")

        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        db_file = Path(vector_db_path) / "vector.db"
        self.vector_db = VectorDatabase(str(db_file))

        # æ­£äº¤æ¨¡å¼è¨­å®š
        self.orthogonal_mode = orthogonal_mode
        self.similarity_threshold = similarity_threshold

        if orthogonal_mode:
            logger.info(f"âœ“ æ­£äº¤æ¨¡å¼å·²å•Ÿç”¨ï¼ˆç›¸ä¼¼åº¦é–¾å€¼: {similarity_threshold}ï¼‰")

        logger.info("âœ“ CodebaseEmbedding å·²åˆå§‹åŒ–")

    def _generate_chunk_id(self, file_path: str, start_line: int, end_line: int) -> str:
        """ç”Ÿæˆç¨‹å¼ç¢¼åˆ†å¡Šçš„å”¯ä¸€ ID

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            start_line: èµ·å§‹è¡Œ
            end_line: çµæŸè¡Œ

        Returns:
            chunk_id (SHA256 hash)
        """
        unique_str = f"{file_path}:{start_line}-{end_line}"
        return hashlib.sha256(unique_str.encode()).hexdigest()[:16]

    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆ embedding

        Args:
            text: æ–‡æœ¬å…§å®¹

        Returns:
            embedding å‘é‡ï¼ˆ768ç¶­ï¼‰æˆ– None
        """
        if not self.has_genai:
            logger.warning("âœ— Gemini API æœªé…ç½®ï¼Œç„¡æ³•ç”Ÿæˆ embedding")
            return None

        try:
            result = self.genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_document"
            )
            return result['embedding']

        except Exception as e:
            logger.error(f"âœ— ç”Ÿæˆ embedding å¤±æ•—: {e}")
            return None

    def _check_orthogonality(self, new_embedding: List[float], chunk_type: Optional[str] = None) -> Tuple[bool, float, Optional[str]]:
        """æª¢æŸ¥æ–° embedding æ˜¯å¦èˆ‡ç¾æœ‰å…§å®¹æ­£äº¤ï¼ˆç·šæ€§ç¨ç«‹ï¼‰

        Args:
            new_embedding: æ–°çš„ embedding å‘é‡
            chunk_type: é™å®šæª¢æŸ¥çš„åˆ†å¡Šé¡å‹ï¼ˆå¯é¸ï¼Œä¾‹å¦‚åªæª¢æŸ¥ 'conversation'ï¼‰

        Returns:
            (æ˜¯å¦æ­£äº¤, æœ€é«˜ç›¸ä¼¼åº¦, æœ€ç›¸ä¼¼çš„ chunk_id)
        """
        if not self.orthogonal_mode:
            return True, 0.0, None

        # ç²å–æ‰€æœ‰ç¾æœ‰ chunks
        all_chunks = self.vector_db.get_all_chunks()

        # éæ¿¾ç‰¹å®šé¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if chunk_type:
            all_chunks = [c for c in all_chunks if c.chunk_type == chunk_type]

        if not all_chunks:
            return True, 0.0, None

        # è¨ˆç®—èˆ‡æ‰€æœ‰ç¾æœ‰ embedding çš„ç›¸ä¼¼åº¦
        new_vec = np.array(new_embedding, dtype=np.float32)
        new_norm = np.linalg.norm(new_vec)

        max_similarity = 0.0
        most_similar_id = None

        for chunk in all_chunks:
            if not chunk.embedding:
                continue

            chunk_vec = np.array(chunk.embedding, dtype=np.float32)
            chunk_norm = np.linalg.norm(chunk_vec)

            # é¤˜å¼¦ç›¸ä¼¼åº¦
            if new_norm > 0 and chunk_norm > 0:
                similarity = float(np.dot(new_vec, chunk_vec) / (new_norm * chunk_norm))

                if similarity > max_similarity:
                    max_similarity = similarity
                    most_similar_id = chunk.chunk_id

        # åˆ¤æ–·æ˜¯å¦æ­£äº¤ï¼ˆç›¸ä¼¼åº¦ä½æ–¼é–¾å€¼ï¼‰
        is_orthogonal = max_similarity < self.similarity_threshold

        return is_orthogonal, max_similarity, most_similar_id

    def _chunk_code_simple(self, file_path: str, content: str, language: str) -> List[CodeChunk]:
        """ç°¡å–®çš„ç¨‹å¼ç¢¼åˆ†å¡Šï¼ˆæŒ‰è¡Œæ•¸ï¼‰

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            content: æª”æ¡ˆå…§å®¹
            language: ç¨‹å¼èªè¨€

        Returns:
            CodeChunk åˆ—è¡¨
        """
        lines = content.split('\n')
        chunks = []

        # æ¯ 50 è¡Œåˆ‡æˆä¸€å€‹ chunk
        chunk_size = 50
        for i in range(0, len(lines), chunk_size):
            chunk_lines = lines[i:i+chunk_size]
            chunk_content = '\n'.join(chunk_lines)

            if not chunk_content.strip():
                continue

            chunk = CodeChunk(
                file_path=file_path,
                chunk_id=self._generate_chunk_id(file_path, i+1, min(i+chunk_size, len(lines))),
                content=chunk_content,
                chunk_type='file',
                start_line=i+1,
                end_line=min(i+chunk_size, len(lines)),
                language=language
            )
            chunks.append(chunk)

        return chunks

    def embed_file(self, file_path: str) -> int:
        """å°å–®å€‹æª”æ¡ˆå»ºç«‹ embedding

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘

        Returns:
            æˆåŠŸå»ºç«‹ embedding çš„åˆ†å¡Šæ•¸é‡
        """
        file_path_obj = Path(file_path)

        if not file_path_obj.exists():
            logger.error(f"âœ— æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            return 0

        # æª¢æŸ¥æª”æ¡ˆå‰¯æª”å
        ext = file_path_obj.suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            logger.warning(f"âš  ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {ext} ({file_path})")
            return 0

        language = self.SUPPORTED_EXTENSIONS[ext]

        try:
            # è®€å–æª”æ¡ˆ
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åˆ†å¡Š
            chunks = self._chunk_code_simple(str(file_path_obj), content, language)

            # ç”Ÿæˆ embedding ä¸¦å„²å­˜
            success_count = 0
            skipped_count = 0
            for chunk in chunks:
                embedding = self._get_embedding(chunk.content)
                if embedding:
                    # æ­£äº¤æ€§æª¢æŸ¥
                    is_orthogonal, max_sim, similar_id = self._check_orthogonality(
                        embedding,
                        chunk_type='file'
                    )

                    if is_orthogonal:
                        chunk.embedding = embedding
                        self.vector_db.add_chunk(chunk)
                        success_count += 1
                    else:
                        skipped_count += 1
                        logger.info(f"âŠ¥ è·³éç›¸ä¼¼å…§å®¹ (ç›¸ä¼¼åº¦: {max_sim:.4f}, èˆ‡ {similar_id} ç›¸ä¼¼)")

            if skipped_count > 0:
                logger.info(f"âœ“ {file_path}: {success_count}/{len(chunks)} chunks embedded (è·³é {skipped_count} å€‹é‡è¤‡å…§å®¹)")
            else:
                logger.info(f"âœ“ {file_path}: {success_count}/{len(chunks)} chunks embedded")
            return success_count

        except Exception as e:
            logger.error(f"âœ— è™•ç†æª”æ¡ˆå¤±æ•— ({file_path}): {e}")
            return 0

    def embed_codebase(
        self,
        root_path: str,
        extensions: Optional[List[str]] = None,
        exclude_dirs: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """å°æ•´å€‹ codebase å»ºç«‹ embedding

        Args:
            root_path: å°ˆæ¡ˆæ ¹ç›®éŒ„
            extensions: è¦è™•ç†çš„å‰¯æª”ååˆ—è¡¨ï¼ˆé è¨­ï¼šæ‰€æœ‰æ”¯æ´çš„èªè¨€ï¼‰
            exclude_dirs: è¦æ’é™¤çš„ç›®éŒ„åˆ—è¡¨ï¼ˆé è¨­ï¼šå¸¸è¦‹çš„æ’é™¤ç›®éŒ„ï¼‰

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        root_path_obj = Path(root_path)

        if not root_path_obj.exists():
            logger.error(f"âœ— ç›®éŒ„ä¸å­˜åœ¨: {root_path}")
            return {'success': False, 'error': 'Directory not found'}

        # é è¨­æ’é™¤ç›®éŒ„
        if exclude_dirs is None:
            exclude_dirs = [
                'node_modules', '__pycache__', '.git', '.venv', 'venv',
                'build', 'dist', '.pytest_cache', '.mypy_cache', 'htmlcov'
            ]

        # é è¨­è™•ç†æ‰€æœ‰æ”¯æ´çš„èªè¨€
        if extensions is None:
            extensions = list(self.SUPPORTED_EXTENSIONS.keys())

        logger.info(f"ğŸ” é–‹å§‹æƒæ codebase: {root_path}")
        logger.info(f"  æ”¯æ´çš„å‰¯æª”å: {', '.join(extensions)}")
        logger.info(f"  æ’é™¤çš„ç›®éŒ„: {', '.join(exclude_dirs)}")

        total_files = 0
        total_chunks = 0
        failed_files = []

        # éæ­·ç›®éŒ„
        for ext in extensions:
            for file_path in root_path_obj.rglob(f'*{ext}'):
                # æª¢æŸ¥æ˜¯å¦åœ¨æ’é™¤ç›®éŒ„ä¸­
                if any(excluded in file_path.parts for excluded in exclude_dirs):
                    continue

                # è™•ç†æª”æ¡ˆ
                chunks_count = self.embed_file(str(file_path))
                if chunks_count > 0:
                    total_files += 1
                    total_chunks += chunks_count
                else:
                    failed_files.append(str(file_path))

        logger.info(f"âœ“ Codebase embedding å®Œæˆï¼")
        logger.info(f"  æˆåŠŸ: {total_files} å€‹æª”æ¡ˆ, {total_chunks} å€‹åˆ†å¡Š")
        if failed_files:
            logger.info(f"  å¤±æ•—: {len(failed_files)} å€‹æª”æ¡ˆ")

        return {
            'success': True,
            'total_files': total_files,
            'total_chunks': total_chunks,
            'failed_files': failed_files,
            'root_path': str(root_path)
        }

    def search_similar_code(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœå°‹ç›¸ä¼¼ç¨‹å¼ç¢¼

        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›å‰ k å€‹çµæœ

        Returns:
            æœå°‹çµæœåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è©¢ embedding
        query_embedding = self._get_embedding(query)
        if not query_embedding:
            logger.error("âœ— ç„¡æ³•ç”ŸæˆæŸ¥è©¢ embedding")
            return []

        # æœå°‹ç›¸ä¼¼åˆ†å¡Š
        results = self.vector_db.search_similar(query_embedding, top_k=top_k)

        # æ ¼å¼åŒ–çµæœ
        formatted_results = []
        for chunk, similarity in results:
            formatted_results.append({
                'file_path': chunk.file_path,
                'chunk_id': chunk.chunk_id,
                'content': chunk.content,
                'chunk_type': chunk.chunk_type,
                'start_line': chunk.start_line,
                'end_line': chunk.end_line,
                'language': chunk.language,
                'similarity': round(similarity, 4)
            })

        return formatted_results

    def add_conversation(
        self,
        question: str,
        answer: str,
        timestamp: Optional[str] = None,
        session_id: Optional[str] = None,
        **metadata
    ) -> str:
        """æ–°å¢å°è©±è¨˜éŒ„åˆ°å‘é‡è³‡æ–™åº«

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            answer: AI å›ç­”
            timestamp: æ™‚é–“æˆ³è¨˜ï¼ˆå¯é¸ï¼‰
            session_id: å°è©± Session IDï¼ˆå¯é¸ï¼‰
            **metadata: å…¶ä»–å…ƒæ•¸æ“š

        Returns:
            chunk_id
        """
        import time
        from datetime import datetime

        # ç”Ÿæˆæ™‚é–“æˆ³è¨˜
        if timestamp is None:
            timestamp = datetime.now().isoformat()

        # å»ºç«‹å°è©±å…§å®¹ï¼ˆå•é¡Œ + å›ç­”ï¼‰
        conversation_content = f"Q: {question}\n\nA: {answer}"

        # ç”Ÿæˆå”¯ä¸€ ID
        chunk_id = self._generate_chunk_id(
            file_path=f"conversation/{session_id or 'default'}",
            start_line=int(time.time()),
            end_line=0
        )

        # å»ºç«‹ CodeChunk
        conversation_chunk = CodeChunk(
            file_path=f"conversation/{session_id or 'default'}",
            chunk_id=chunk_id,
            content=conversation_content,
            chunk_type='conversation',
            start_line=0,
            end_line=0,
            language='natural_language',
            metadata={
                'timestamp': timestamp,
                'session_id': session_id,
                'question': question,
                'answer': answer,
                **metadata
            }
        )

        # ç”Ÿæˆ embedding
        embedding = self._get_embedding(conversation_content)
        if embedding:
            # æ­£äº¤æ€§æª¢æŸ¥ï¼ˆåªæª¢æŸ¥å°è©±é¡å‹ï¼‰
            is_orthogonal, max_sim, similar_id = self._check_orthogonality(
                embedding,
                chunk_type='conversation'
            )

            if is_orthogonal:
                conversation_chunk.embedding = embedding
                self.vector_db.add_chunk(conversation_chunk)
                logger.info(f"âœ“ å°è©±è¨˜éŒ„å·²æ–°å¢: {chunk_id}")
                return chunk_id
            else:
                logger.warning(f"âŠ¥ é‡è¤‡å°è©±å…§å®¹ (ç›¸ä¼¼åº¦: {max_sim:.4f}, èˆ‡ {similar_id} ç›¸ä¼¼)")
                logger.info(f"  æç¤ºï¼šæ­¤å°è©±èˆ‡å·²å­˜åœ¨çš„å°è©±éæ–¼ç›¸ä¼¼ï¼Œå·²è·³é")
                return ""  # è¿”å›ç©ºå­—ä¸²è¡¨ç¤ºæœªæ–°å¢
        else:
            logger.error("âœ— ç„¡æ³•ç”Ÿæˆå°è©± embedding")
            return ""

    def search_conversations(
        self,
        query: str,
        top_k: int = 5,
        session_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æœå°‹ç›¸é—œå°è©±è¨˜éŒ„

        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›å‰ k å€‹çµæœ
            session_id: é™å®šç‰¹å®š Sessionï¼ˆå¯é¸ï¼‰

        Returns:
            æœå°‹çµæœåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è©¢ embedding
        query_embedding = self._get_embedding(query)
        if not query_embedding:
            logger.error("âœ— ç„¡æ³•ç”ŸæˆæŸ¥è©¢ embedding")
            return []

        # æœå°‹ç›¸ä¼¼å°è©±
        all_results = self.vector_db.search_similar(query_embedding, top_k=top_k * 2)

        # éæ¿¾å°è©±é¡å‹
        conversation_results = []
        for chunk, similarity in all_results:
            if chunk.chunk_type != 'conversation':
                continue

            # å¦‚æœæŒ‡å®šäº† session_idï¼Œå‰‡é€²ä¸€æ­¥éæ¿¾
            if session_id and chunk.metadata:
                if chunk.metadata.get('session_id') != session_id:
                    continue

            conversation_results.append({
                'chunk_id': chunk.chunk_id,
                'question': chunk.metadata.get('question') if chunk.metadata else '',
                'answer': chunk.metadata.get('answer') if chunk.metadata else '',
                'timestamp': chunk.metadata.get('timestamp') if chunk.metadata else '',
                'session_id': chunk.metadata.get('session_id') if chunk.metadata else '',
                'similarity': round(similarity, 4),
                'metadata': chunk.metadata
            })

            if len(conversation_results) >= top_k:
                break

        return conversation_results

    def analyze_orthogonality(self, chunk_type: Optional[str] = None) -> Dict[str, Any]:
        """åˆ†æè³‡æ–™åº«ä¸­å‘é‡çš„æ­£äº¤æ€§ï¼ˆç·šæ€§ç¨ç«‹æ€§ï¼‰

        Args:
            chunk_type: é™å®šåˆ†æçš„åˆ†å¡Šé¡å‹ï¼ˆå¯é¸ï¼‰

        Returns:
            æ­£äº¤æ€§åˆ†æå ±å‘Š
        """
        all_chunks = self.vector_db.get_all_chunks()

        # éæ¿¾ç‰¹å®šé¡å‹
        if chunk_type:
            all_chunks = [c for c in all_chunks if c.chunk_type == chunk_type]

        if len(all_chunks) < 2:
            return {
                'total_chunks': len(all_chunks),
                'chunk_type': chunk_type or 'all',
                'message': 'è³‡æ–™ä¸è¶³ï¼ˆéœ€è‡³å°‘ 2 å€‹ chunksï¼‰'
            }

        # æå–æ‰€æœ‰ embeddings
        embeddings = []
        chunk_ids = []
        for chunk in all_chunks:
            if chunk.embedding:
                embeddings.append(np.array(chunk.embedding, dtype=np.float32))
                chunk_ids.append(chunk.chunk_id)

        if len(embeddings) < 2:
            return {
                'total_chunks': len(all_chunks),
                'chunks_with_embedding': len(embeddings),
                'chunk_type': chunk_type or 'all',
                'message': 'æœ‰æ•ˆ embedding ä¸è¶³'
            }

        # è¨ˆç®—æ‰€æœ‰å‘é‡ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦
        n = len(embeddings)
        similarity_matrix = np.zeros((n, n))

        for i in range(n):
            for j in range(i+1, n):
                vec_i = embeddings[i]
                vec_j = embeddings[j]

                norm_i = np.linalg.norm(vec_i)
                norm_j = np.linalg.norm(vec_j)

                if norm_i > 0 and norm_j > 0:
                    similarity = float(np.dot(vec_i, vec_j) / (norm_i * norm_j))
                    similarity_matrix[i, j] = similarity
                    similarity_matrix[j, i] = similarity

        # çµ±è¨ˆåˆ†æ
        upper_triangle = similarity_matrix[np.triu_indices(n, k=1)]
        mean_similarity = float(np.mean(upper_triangle))
        max_similarity = float(np.max(upper_triangle))
        min_similarity = float(np.min(upper_triangle))
        std_similarity = float(np.std(upper_triangle))

        # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å‘é‡å°
        max_idx = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)
        most_similar_pair = (chunk_ids[max_idx[0]], chunk_ids[max_idx[1]], max_similarity)

        # è¨ˆç®—æ­£äº¤åº¦ï¼ˆç·šæ€§ç¨ç«‹æ€§ï¼‰
        # æ­£äº¤åº¦ = 1 - å¹³å‡ç›¸ä¼¼åº¦ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šæ­£äº¤ï¼‰
        orthogonality_score = 1.0 - mean_similarity

        # çµ±è¨ˆç›¸ä¼¼åº¦åˆ†ä½ˆ
        high_similarity_count = int(np.sum(upper_triangle > 0.85))
        medium_similarity_count = int(np.sum((upper_triangle > 0.5) & (upper_triangle <= 0.85)))
        low_similarity_count = int(np.sum(upper_triangle <= 0.5))

        return {
            'total_chunks': n,
            'chunk_type': chunk_type or 'all',
            'orthogonality_score': round(orthogonality_score, 4),
            'similarity_stats': {
                'mean': round(mean_similarity, 4),
                'max': round(max_similarity, 4),
                'min': round(min_similarity, 4),
                'std': round(std_similarity, 4)
            },
            'most_similar_pair': {
                'chunk_1': most_similar_pair[0],
                'chunk_2': most_similar_pair[1],
                'similarity': round(most_similar_pair[2], 4)
            },
            'similarity_distribution': {
                'high (>0.85)': high_similarity_count,
                'medium (0.5-0.85)': medium_similarity_count,
                'low (<0.5)': low_similarity_count
            },
            'interpretation': self._interpret_orthogonality(orthogonality_score)
        }

    def _interpret_orthogonality(self, score: float) -> str:
        """è§£é‡‹æ­£äº¤åº¦åˆ†æ•¸

        Args:
            score: æ­£äº¤åº¦åˆ†æ•¸ (0-1)

        Returns:
            è§£é‡‹æ–‡å­—
        """
        if score > 0.8:
            return "å„ªç§€ï¼šå…§å®¹é«˜åº¦ç·šæ€§ç¨ç«‹ï¼Œå¹¾ä¹ç„¡é‡è¤‡"
        elif score > 0.6:
            return "è‰¯å¥½ï¼šå…§å®¹å¤§å¤šç¨ç«‹ï¼Œå°‘é‡ç›¸ä¼¼"
        elif score > 0.4:
            return "ä¸­ç­‰ï¼šå­˜åœ¨ä¸€å®šç¨‹åº¦çš„é‡è¤‡å…§å®¹"
        elif score > 0.2:
            return "è¼ƒå·®ï¼šé‡è¤‡å…§å®¹è¼ƒå¤šï¼Œå»ºè­°æ¸…ç†"
        else:
            return "æ¥µå·®ï¼šå¤§é‡é‡è¤‡å…§å®¹ï¼Œå¼·çƒˆå»ºè­°å•Ÿç”¨æ­£äº¤æ¨¡å¼"

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        stats = self.vector_db.get_stats()
        stats['orthogonal_mode'] = self.orthogonal_mode
        stats['similarity_threshold'] = self.similarity_threshold
        return stats

    def close(self):
        """é—œé–‰è³‡æº"""
        self.vector_db.close()
        logger.info("âœ“ CodebaseEmbedding å·²é—œé–‰")


# æ¨¡çµ„ç´šåˆ¥å‡½æ•¸ï¼ˆæ–¹ä¾¿ä½¿ç”¨ï¼‰
def create_codebase_embedding(
    root_path: str,
    api_key: Optional[str] = None,
    vector_db_path: str = ".embeddings"
) -> CodebaseEmbedding:
    """å»ºç«‹ Codebase Embedding å¯¦ä¾‹ä¸¦è™•ç†æ•´å€‹ codebase

    Args:
        root_path: å°ˆæ¡ˆæ ¹ç›®éŒ„
        api_key: Gemini API Key
        vector_db_path: å‘é‡è³‡æ–™åº«è·¯å¾‘

    Returns:
        CodebaseEmbedding å¯¦ä¾‹
    """
    embedding = CodebaseEmbedding(vector_db_path=vector_db_path, api_key=api_key)
    embedding.embed_codebase(root_path)
    return embedding


if __name__ == "__main__":
    # æ¸¬è©¦ç”¨ä¾‹
    import sys

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("=" * 60)
    print("CodeGemini Codebase Embedding æ¸¬è©¦")
    print("=" * 60)

    # æ¸¬è©¦ VectorDatabase
    print("\n1. æ¸¬è©¦ VectorDatabase")
    db = VectorDatabase(".test_embeddings/vector.db")

    # å»ºç«‹æ¸¬è©¦åˆ†å¡Š
    test_chunk = CodeChunk(
        file_path="test.py",
        chunk_id="test123",
        content="def hello():\n    print('Hello')",
        chunk_type="function",
        start_line=1,
        end_line=2,
        language="python",
        embedding=[0.1, 0.2, 0.3]
    )

    db.add_chunk(test_chunk)
    print(f"âœ“ æ–°å¢æ¸¬è©¦åˆ†å¡Š: {test_chunk.chunk_id}")

    # ç²å–åˆ†å¡Š
    retrieved_chunk = db.get_chunk("test123")
    print(f"âœ“ ç²å–åˆ†å¡Š: {retrieved_chunk.chunk_id if retrieved_chunk else 'None'}")

    # çµ±è¨ˆè³‡è¨Š
    stats = db.get_stats()
    print(f"âœ“ çµ±è¨ˆè³‡è¨Š: {stats}")

    db.close()

    print("\nâœ“ æ‰€æœ‰æ¸¬è©¦é€šéï¼")
