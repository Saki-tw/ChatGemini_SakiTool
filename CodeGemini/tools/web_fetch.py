#!/usr/bin/env python3
"""

# i18n support
import sys
from pathlib import Path

# ç¢ºä¿å¯ä»¥ import utils
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from utils.i18n import safe_t
CodeGemini Web Fetch Module
ç¶²é æŠ“å–å·¥å…· - æä¾›ç¶²é å…§å®¹æŠ“å–åŠŸèƒ½

æ­¤æ¨¡çµ„è² è²¬ï¼š
1. æŠ“å– URL å…§å®¹
2. HTML â†’ Markdown è½‰æ›
3. Redirect è™•ç†
4. å¿«å–ç®¡ç†ï¼ˆ15 åˆ†é˜è‡ªæ¸…ç†ï¼‰
"""

import os
import re
import time
import hashlib
import requests
from typing import Optional, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from cachetools import TTLCache
from rich.console import Console
import html2text

console = Console()


@dataclass
class FetchedPage:
    """æŠ“å–çš„ç¶²é """
    url: str
    title: str
    content: str  # Markdown æ ¼å¼
    raw_html: str
    status_code: int
    headers: Dict[str, str] = field(default_factory=dict)
    redirected_from: Optional[str] = None
    fetched_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def word_count(self) -> int:
        """å­—æ•¸çµ±è¨ˆ"""
        return len(self.content.split())

    @property
    def is_redirect(self) -> bool:
        """æ˜¯å¦ç‚ºé‡å®šå‘"""
        return self.redirected_from is not None


class WebFetcher:
    """
    ç¶²é æŠ“å–å·¥å…·

    åŠŸèƒ½ï¼š
    - æŠ“å–ç¶²é å…§å®¹ä¸¦è½‰æ›ç‚º Markdown
    - è™•ç† HTTP é‡å®šå‘
    - è‡ªå‹•å¿«å–ï¼ˆ15 åˆ†é˜ TTLï¼‰
    - å®‰å…¨æª¢æŸ¥èˆ‡éŒ¯èª¤è™•ç†
    """

    def __init__(
        self,
        timeout: int = 30,
        max_retries: int = 3,
        cache_ttl: int = 900,  # 15 åˆ†é˜ = 900 ç§’
        user_agent: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–ç¶²é æŠ“å–å·¥å…·

        Args:
            timeout: è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
            cache_ttl: å¿«å–ç”Ÿå­˜æ™‚é–“ï¼ˆç§’ï¼‰
            user_agent: è‡ªè¨‚ User-Agent
        """
        self.timeout = timeout
        self.max_retries = max_retries
        self.user_agent = user_agent or (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )

        # åˆå§‹åŒ–å¿«å–ï¼ˆTTL Cacheï¼Œ15 åˆ†é˜è‡ªå‹•æ¸…ç†ï¼‰
        self.cache = TTLCache(maxsize=100, ttl=cache_ttl)

        # åˆå§‹åŒ– HTML è½‰ Markdown è½‰æ›å™¨
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = False
        self.html_converter.body_width = 0  # ä¸æ›è¡Œ
        self.html_converter.unicode_snob = True  # Unicode æ”¯æ´
        self.html_converter.ignore_emphasis = False

        console.print(f"[dim]{safe_t('web_fetch.fetcher_initialized', 'WebFetcher åˆå§‹åŒ–å®Œæˆ')}ï¼ˆ{safe_t('web_fetch.cache_ttl', 'å¿«å– TTL')}: {cache_ttl}{safe_t('common.seconds', 'ç§’')}ï¼‰[/dim]")

    def fetch(
        self,
        url: str,
        use_cache: bool = True,
        follow_redirects: bool = True
    ) -> Optional[FetchedPage]:
        """
        æŠ“å–ç¶²é 

        Args:
            url: ç›®æ¨™ URL
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–
            follow_redirects: æ˜¯å¦è¿½è¹¤é‡å®šå‘

        Returns:
            Optional[FetchedPage]: æŠ“å–çš„ç¶²é ï¼Œå¤±æ•—è¿”å› None
        """
        console.print(f"\n[#B565D8]ğŸŒ {safe_t('web_fetch.fetching', 'æŠ“å–ç¶²é ')}ï¼š{url}[/#B565D8]")

        # æª¢æŸ¥å¿«å–
        if use_cache:
            cached_page = self._get_from_cache(url)
            if cached_page:
                console.print(f"[#B565D8]âœ“ {safe_t('web_fetch.from_cache', 'å¾å¿«å–è®€å–')}[/#B565D8]")
                return cached_page

        # é©—è­‰ URL
        if not self._is_valid_url(url):
            console.print(f"[dim #B565D8]âœ— {safe_t('web_fetch.invalid_url', 'ç„¡æ•ˆçš„ URL')}ï¼š{url}[/red]")
            return None

        # åŸ·è¡ŒæŠ“å–
        try:
            response = self._make_request(url, follow_redirects)

            if response is None:
                return None

            # æª¢æŸ¥ç‹€æ…‹ç¢¼
            if response.status_code != 200:
                console.print(f"[#B565D8]âš ï¸  HTTP {response.status_code}[/#B565D8]")

            # æå–é‡å®šå‘è³‡è¨Š
            redirected_from = None
            if response.history:
                redirected_from = response.history[0].url
                console.print(f"[#B565D8]â†ªï¸  {safe_t('web_fetch.redirected_from', 'é‡å®šå‘è‡ª')}ï¼š{redirected_from}[/#B565D8]")

            # è½‰æ› HTML ç‚º Markdown
            html_content = response.text
            markdown_content = self._convert_html_to_markdown(html_content)

            # æå–æ¨™é¡Œ
            title = self._extract_title(html_content)

            # å»ºç«‹ FetchedPage
            page = FetchedPage(
                url=response.url,
                title=title,
                content=markdown_content,
                raw_html=html_content,
                status_code=response.status_code,
                headers=dict(response.headers),
                redirected_from=redirected_from,
                metadata={
                    "content_type": response.headers.get("Content-Type", ""),
                    "encoding": response.encoding
                }
            )

            # å„²å­˜åˆ°å¿«å–
            if use_cache:
                self._save_to_cache(url, page)

            console.print(f"[#B565D8]âœ“ {safe_t('web_fetch.fetch_success', 'æŠ“å–æˆåŠŸ')}[/#B565D8]")
            console.print(f"  {safe_t('web_fetch.title', 'æ¨™é¡Œ')}ï¼š{title}")
            console.print(f"  {safe_t('web_fetch.word_count', 'å­—æ•¸')}ï¼š{page.word_count}")

            return page

        except Exception as e:
            console.print(f"[dim #B565D8]âœ— {safe_t('web_fetch.fetch_failed', 'æŠ“å–å¤±æ•—')}ï¼š{e}[/red]")
            return None

    def _make_request(
        self,
        url: str,
        follow_redirects: bool
    ) -> Optional[requests.Response]:
        """ç™¼é€ HTTP è«‹æ±‚"""
        headers = {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive"
        }

        for attempt in range(self.max_retries):
            try:
                response = requests.get(
                    url,
                    headers=headers,
                    timeout=self.timeout,
                    allow_redirects=follow_redirects
                )

                return response

            except requests.exceptions.Timeout:
                console.print(f"[#B565D8]âš ï¸  {safe_t('web_fetch.request_timeout', 'è«‹æ±‚è¶…æ™‚')}ï¼ˆ{safe_t('web_fetch.attempt', 'å˜—è©¦')} {attempt + 1}/{self.max_retries}ï¼‰[/#B565D8]")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    continue
                else:
                    return None

            except requests.exceptions.RequestException as e:
                console.print(f"[dim #B565D8]âœ— {safe_t('web_fetch.request_error', 'è«‹æ±‚éŒ¯èª¤')}ï¼š{e}[/red]")
                return None

        return None

    def _convert_html_to_markdown(self, html: str) -> str:
        """å°‡ HTML è½‰æ›ç‚º Markdown"""
        try:
            # æ¸…ç† HTML
            html = self._clean_html(html)

            # è½‰æ›ç‚º Markdown
            markdown = self.html_converter.handle(html)

            # å¾Œè™•ç†
            markdown = self._post_process_markdown(markdown)

            return markdown

        except Exception as e:
            console.print(f"[#B565D8]âš ï¸  {safe_t('web_fetch.markdown_error', 'Markdown è½‰æ›éŒ¯èª¤')}ï¼š{e}[/#B565D8]")
            # å›é€€åˆ°ç´”æ–‡å­—
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            return soup.get_text()

    def _clean_html(self, html: str) -> str:
        """æ¸…ç† HTMLï¼ˆç§»é™¤ scriptã€style ç­‰ï¼‰"""
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„æ¨™ç±¤
        for tag in soup(['script', 'style', 'nav', 'footer', 'aside', 'noscript']):
            tag.decompose()

        return str(soup)

    def _post_process_markdown(self, markdown: str) -> str:
        """å¾Œè™•ç† Markdownï¼ˆæ¸…ç†å¤šé¤˜ç©ºè¡Œç­‰ï¼‰"""
        # ç§»é™¤é€£çºŒçš„å¤šå€‹ç©ºè¡Œ
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)

        # ç§»é™¤è¡Œå°¾ç©ºç™½
        markdown = re.sub(r'[ \t]+\n', '\n', markdown)

        # ç§»é™¤é–‹é ­å’Œçµå°¾ç©ºç™½
        markdown = markdown.strip()

        return markdown

    def _extract_title(self, html: str) -> str:
        """å¾ HTML æå–æ¨™é¡Œ"""
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html, 'html.parser')

        # å˜—è©¦å¾ <title> æå–
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text(strip=True)

        # å˜—è©¦å¾ <h1> æå–
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text(strip=True)

        # å˜—è©¦å¾ og:title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content']

        return safe_t('web_fetch.no_title', 'ï¼ˆç„¡æ¨™é¡Œï¼‰')

    def _is_valid_url(self, url: str) -> bool:
        """é©—è­‰ URL æ ¼å¼"""
        # ç°¡å–®çš„ URL é©—è­‰
        pattern = re.compile(
            r'^https?://'  # http:// æˆ– https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # åŸŸå
            r'localhost|'  # localhost
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # IP
            r'(?::\d+)?'  # å¯é¸ç«¯å£
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)

        return bool(pattern.match(url))

    def _get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆå¿«å–éµ"""
        return hashlib.md5(url.encode()).hexdigest()

    def _get_from_cache(self, url: str) -> Optional[FetchedPage]:
        """å¾å¿«å–è®€å–"""
        key = self._get_cache_key(url)
        return self.cache.get(key)

    def _save_to_cache(self, url: str, page: FetchedPage) -> None:
        """å„²å­˜åˆ°å¿«å–"""
        key = self._get_cache_key(url)
        self.cache[key] = page

    def clear_cache(self) -> None:
        """æ¸…ç©ºå¿«å–"""
        self.cache.clear()
        console.print(f"[#B565D8]âœ“ {safe_t('web_fetch.cache_cleared', 'å¿«å–å·²æ¸…ç©º')}[/#B565D8]")

    def get_cache_stats(self) -> Dict[str, Any]:
        """å–å¾—å¿«å–çµ±è¨ˆ"""
        return {
            "size": len(self.cache),
            "maxsize": self.cache.maxsize,
            "ttl": self.cache.ttl,
            "items": list(self.cache.keys())
        }


# ==================== å‘½ä»¤åˆ—ä»‹é¢ ====================

def main():
    """Web Fetch å‘½ä»¤åˆ—å·¥å…·"""
    import sys

    console.print("\n[bold #B565D8]CodeGemini Web Fetch Tool[/bold #B565D8]\n")

    if len(sys.argv) < 2:
        console.print(f"{safe_t('common.usage', 'ç”¨æ³•')}ï¼š")
        console.print("  python tools/web_fetch.py <url> [--no-cache] [--output <file>]")
        console.print(f"\n{safe_t('common.options', 'é¸é …')}ï¼š")
        console.print(f"  --no-cache  {safe_t('web_fetch.no_cache', 'ä¸ä½¿ç”¨å¿«å–')}")
        console.print(f"  --output    {safe_t('web_fetch.output_to_file', 'è¼¸å‡ºåˆ°æª”æ¡ˆ')}")
        console.print(f"\n{safe_t('common.examples', 'ç¯„ä¾‹')}ï¼š")
        console.print("  python tools/web_fetch.py https://example.com")
        console.print("  python tools/web_fetch.py https://example.com --output page.md")
        return

    url = sys.argv[1]
    use_cache = True
    output_file = None

    # è§£æåƒæ•¸
    for i, arg in enumerate(sys.argv):
        if arg == "--no-cache":
            use_cache = False
        elif arg == "--output" and i + 1 < len(sys.argv):
            output_file = sys.argv[i + 1]

    # åŸ·è¡ŒæŠ“å–
    fetcher = WebFetcher()
    page = fetcher.fetch(url, use_cache=use_cache)

    if page:
        # é¡¯ç¤ºçµæœ
        console.print(f"\n[bold]ğŸ“„ {safe_t('web_fetch.page_content', 'ç¶²é å…§å®¹')}ï¼š[/bold]\n")
        console.print(f"[bold #B565D8]{safe_t('web_fetch.title', 'æ¨™é¡Œ')}ï¼š[/bold #B565D8]{page.title}")
        console.print(f"[bold #B565D8]URLï¼š[/bold #B565D8]{page.url}")
        console.print(f"[bold #B565D8]{safe_t('web_fetch.word_count', 'å­—æ•¸')}ï¼š[/bold #B565D8]{page.word_count}")
        console.print(f"[bold #B565D8]{safe_t('web_fetch.status_code', 'ç‹€æ…‹ç¢¼')}ï¼š[/bold #B565D8]{page.status_code}")

        if page.is_redirect:
            console.print(f"[bold #B565D8]{safe_t('web_fetch.redirected_from', 'é‡å®šå‘è‡ª')}ï¼š[/bold #B565D8]{page.redirected_from}")

        console.print(f"\n[dim]--- {safe_t('web_fetch.markdown_content', 'Markdown å…§å®¹')}ï¼ˆ{safe_t('web_fetch.first_chars', 'å‰')} 500 {safe_t('common.characters', 'å­—')}ï¼‰ ---[/dim]")
        console.print(page.content[:500] + "..." if len(page.content) > 500 else page.content)

        # è¼¸å‡ºåˆ°æª”æ¡ˆ
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"# {page.title}\n\n")
                f.write(f"**URL:** {page.url}\n\n")
                f.write(f"**{safe_t('web_fetch.fetch_time', 'æŠ“å–æ™‚é–“')}:** {page.fetched_at.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("---\n\n")
                f.write(page.content)

            console.print(f"\n[#B565D8]âœ“ {safe_t('web_fetch.saved_to', 'å·²å„²å­˜åˆ°')}ï¼š{output_file}[/#B565D8]")
    else:
        console.print(f"[dim #B565D8]âœ— {safe_t('web_fetch.fetch_failed', 'æŠ“å–å¤±æ•—')}[/red]")


if __name__ == "__main__":
    main()
