#!/usr/bin/env python3
"""
CodeGemini Smart Context Builder Module
æ™ºèƒ½ä¸Šä¸‹æ–‡å»ºæ§‹å™¨ - ç‚ºä»»å‹™é¸æ“‡æœ€ç›¸é—œçš„ç¨‹å¼ç¢¼ä¸Šä¸‹æ–‡

æ­¤æ¨¡çµ„è² è²¬ï¼š
1. å»ºç«‹ä»»å‹™å°ˆå±¬çš„ä¸Šä¸‹æ–‡
2. æª”æ¡ˆç›¸é—œæ€§è©•åˆ†èˆ‡å„ªå…ˆæ’åº
3. æå–ç›¸é—œç¨‹å¼ç¢¼ç‰‡æ®µ
4. Token é ç®—ç®¡ç†
5. æ¼¸é€²å¼ä¸Šä¸‹æ–‡è¼‰å…¥
6. ä¸Šä¸‹æ–‡å£“ç¸®ç­–ç•¥
"""
import os
import re
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass, field
from pathlib import Path
from enum import Enum
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

from .scanner import ProjectContext, CodebaseScanner
from utils.i18n import safe_t

console = Console()


class RelevanceLevel(Enum):
    """ç›¸é—œæ€§ç­‰ç´š"""
    CRITICAL = "critical"      # é—œéµï¼ˆ>= 0.8ï¼‰
    HIGH = "high"              # é«˜ï¼ˆ>= 0.6ï¼‰
    MEDIUM = "medium"          # ä¸­ï¼ˆ>= 0.4ï¼‰
    LOW = "low"                # ä½ï¼ˆ>= 0.2ï¼‰
    MINIMAL = "minimal"        # æœ€ä½ï¼ˆ< 0.2ï¼‰


@dataclass
class CodeSnippet:
    """ç¨‹å¼ç¢¼ç‰‡æ®µ"""
    file_path: str                      # æª”æ¡ˆè·¯å¾‘
    start_line: int                     # èµ·å§‹è¡Œè™Ÿ
    end_line: int                       # çµæŸè¡Œè™Ÿ
    content: str                        # å…§å®¹
    relevance_score: float = 0.0        # ç›¸é—œæ€§åˆ†æ•¸
    context_type: str = "code"          # é¡å‹ï¼ˆcode, import, class, functionï¼‰
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class FileContext:
    """æª”æ¡ˆä¸Šä¸‹æ–‡"""
    file_path: str                      # æª”æ¡ˆè·¯å¾‘
    relevance_score: float              # ç›¸é—œæ€§åˆ†æ•¸
    snippets: List[CodeSnippet] = field(default_factory=list)
    full_content: Optional[str] = None  # å®Œæ•´å…§å®¹ï¼ˆé¸ç”¨ï¼‰
    estimated_tokens: int = 0           # é ä¼° token æ•¸
    priority: int = 0                   # å„ªå…ˆç´š


@dataclass
class Context:
    """ä»»å‹™ä¸Šä¸‹æ–‡"""
    task_description: str               # ä»»å‹™æè¿°
    project_context: ProjectContext     # å°ˆæ¡ˆä¸Šä¸‹æ–‡
    file_contexts: List[FileContext] = field(default_factory=list)
    total_tokens: int = 0               # ç¸½ token æ•¸
    token_budget: int = 100000          # Token é ç®—ï¼ˆé è¨­ 100kï¼‰
    included_files: int = 0             # åŒ…å«çš„æª”æ¡ˆæ•¸
    metadata: Dict[str, Any] = field(default_factory=dict)


class ContextBuilder:
    """æ™ºèƒ½ä¸Šä¸‹æ–‡å»ºæ§‹å™¨"""

    # Token ä¼°ç®—å¸¸æ•¸ï¼ˆç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—å…ƒï¼‰
    CHARS_PER_TOKEN = 4

    # æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ï¼ˆGemini 2.0 Flashï¼š1M tokensï¼‰
    MAX_CONTEXT_TOKENS = 1000000

    def __init__(
        self,
        project_path: str,
        token_budget: int = 100000,
        scanner: Optional[CodebaseScanner] = None
    ):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡å»ºæ§‹å™¨

        Args:
            project_path: å°ˆæ¡ˆè·¯å¾‘
            token_budget: Token é ç®—
            scanner: ç¨‹å¼ç¢¼åº«æƒæå™¨ï¼ˆé¸ç”¨ï¼‰
        """
        self.project_path = os.path.abspath(project_path)
        self.token_budget = min(token_budget, self.MAX_CONTEXT_TOKENS)
        self.scanner = scanner or CodebaseScanner()

        # æƒæå°ˆæ¡ˆï¼ˆä½¿ç”¨å¿«å–ï¼‰
        self.project_context: Optional[ProjectContext] = None

    def build_for_task(
        self,
        task_description: str,
        keywords: Optional[List[str]] = None,
        max_files: int = 20,
        include_tests: bool = False
    ) -> Context:
        """
        ç‚ºä»»å‹™å»ºç«‹ä¸Šä¸‹æ–‡

        Args:
            task_description: ä»»å‹™æè¿°
            keywords: é—œéµå­—åˆ—è¡¨ï¼ˆé¸ç”¨ï¼‰
            max_files: æœ€å¤§æª”æ¡ˆæ•¸
            include_tests: æ˜¯å¦åŒ…å«æ¸¬è©¦æª”æ¡ˆ

        Returns:
            Context: å»ºç«‹çš„ä¸Šä¸‹æ–‡
        """
        console.print(safe_t("context.build.starting", fallback="\n[#B565D8]ğŸ”¨ å»ºç«‹ä»»å‹™ä¸Šä¸‹æ–‡...[/#B565D8]"))
        console.print(safe_t("context.build.task", fallback="  ä»»å‹™ï¼š{task}...").format(task=task_description[:60]))

        # æ­¥é©Ÿ 1ï¼šæƒæå°ˆæ¡ˆï¼ˆå¦‚æœé‚„æ²’æƒæï¼‰
        if not self.project_context:
            self.project_context = self.scanner.scan_project(
                self.project_path,
                build_symbol_index=False
            )

        # æ­¥é©Ÿ 2ï¼šæå–é—œéµå­—
        if not keywords:
            keywords = self._extract_keywords(task_description)

        console.print(safe_t("context.build.keywords", fallback="  é—œéµå­—ï¼š{keywords}{more}").format(keywords=', '.join(keywords[:5]), more='...' if len(keywords) > 5 else ''))

        # æ­¥é©Ÿ 3ï¼šç²å–å€™é¸æª”æ¡ˆ
        candidate_files = self.project_context.source_files
        if include_tests:
            candidate_files = candidate_files + self.project_context.test_files

        # æ­¥é©Ÿ 4ï¼šæª”æ¡ˆå„ªå…ˆç´šæ’åº
        prioritized_files = self.prioritize_files(
            task_description,
            candidate_files,
            keywords
        )

        # æ­¥é©Ÿ 5ï¼šå»ºç«‹æª”æ¡ˆä¸Šä¸‹æ–‡ï¼ˆæ¼¸é€²å¼è¼‰å…¥ï¼‰
        file_contexts = self._build_file_contexts(
            prioritized_files[:max_files],
            keywords,
            self.token_budget
        )

        # æ­¥é©Ÿ 6ï¼šè¨ˆç®—ç¸½ token æ•¸
        total_tokens = sum(fc.estimated_tokens for fc in file_contexts)

        # æ­¥é©Ÿ 7ï¼šå»ºç«‹ä¸Šä¸‹æ–‡
        context = Context(
            task_description=task_description,
            project_context=self.project_context,
            file_contexts=file_contexts,
            total_tokens=total_tokens,
            token_budget=self.token_budget,
            included_files=len(file_contexts),
            metadata={
                'keywords': keywords,
                'max_files': max_files
            }
        )

        console.print(safe_t("context.build.completed", fallback="[#B565D8]âœ“ ä¸Šä¸‹æ–‡å·²å»ºç«‹[/#B565D8]"))
        console.print(safe_t("context.build.files", fallback="  åŒ…å«æª”æ¡ˆï¼š{count}").format(count=context.included_files))
        console.print(safe_t("context.build.tokens", fallback="  é ä¼° tokensï¼š{tokens:,}").format(tokens=context.total_tokens))
        console.print(safe_t("context.build.usage", fallback="  é ç®—ä½¿ç”¨ç‡ï¼š{usage:.1f}%").format(usage=context.total_tokens / context.token_budget * 100))

        return context

    def prioritize_files(
        self,
        task_description: str,
        files: List[str],
        keywords: List[str]
    ) -> List[str]:
        """
        æª”æ¡ˆå„ªå…ˆç´šæ’åº

        Args:
            task_description: ä»»å‹™æè¿°
            files: æª”æ¡ˆåˆ—è¡¨
            keywords: é—œéµå­—åˆ—è¡¨

        Returns:
            List[str]: æ’åºå¾Œçš„æª”æ¡ˆåˆ—è¡¨ï¼ˆç”±é«˜åˆ°ä½ï¼‰
        """
        console.print(safe_t("context.relevance.calculating", fallback="\n[#B565D8]ğŸ“Š è¨ˆç®—æª”æ¡ˆç›¸é—œæ€§...[/#B565D8]"))

        file_scores: List[Tuple[str, float]] = []

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            console=console,
        ) as progress:
            task = progress.add_task("è©•åˆ†ä¸­...", total=len(files))

            for file_path in files:
                score = self._calculate_file_relevance(
                    file_path,
                    task_description,
                    keywords
                )
                file_scores.append((file_path, score))
                progress.update(task, advance=1)

        # æ’åºï¼ˆåˆ†æ•¸ç”±é«˜åˆ°ä½ï¼‰
        file_scores.sort(key=lambda x: x[1], reverse=True)

        # é¡¯ç¤ºå‰ 10 å€‹æœ€ç›¸é—œçš„æª”æ¡ˆ
        console.print(safe_t("context.relevance.completed", fallback="[#B565D8]âœ“ ç›¸é—œæ€§è©•åˆ†å®Œæˆ[/#B565D8]"))
        console.print(safe_t("context.relevance.top10", fallback="\n[#B565D8]å‰ 10 å€‹æœ€ç›¸é—œæª”æ¡ˆï¼š[/#B565D8]"))
        for i, (file, score) in enumerate(file_scores[:10], 1):
            level = self._get_relevance_level(score)
            console.print(f"  {i}. [{level.value}] {os.path.basename(file)} ({score:.2f})")

        return [file for file, _ in file_scores]

    def extract_relevant_code(
        self,
        file_path: str,
        keywords: List[str],
        max_snippets: int = 5
    ) -> List[CodeSnippet]:
        """
        æå–ç›¸é—œç¨‹å¼ç¢¼ç‰‡æ®µ

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            keywords: é—œéµå­—åˆ—è¡¨
            max_snippets: æœ€å¤§ç‰‡æ®µæ•¸

        Returns:
            List[CodeSnippet]: ç¨‹å¼ç¢¼ç‰‡æ®µåˆ—è¡¨
        """
        full_path = os.path.join(self.project_path, file_path)

        if not os.path.exists(full_path):
            return []

        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            snippets = []

            # ç°¡å–®ç­–ç•¥ï¼šæ‰¾å‡ºåŒ…å«é—œéµå­—çš„è¡ŒåŠå…¶ä¸Šä¸‹æ–‡
            for i, line in enumerate(lines):
                line_lower = line.lower()

                # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµå­—
                matches = [kw for kw in keywords if kw.lower() in line_lower]

                if matches:
                    # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰å¾Œ 5 è¡Œï¼‰
                    start = max(0, i - 5)
                    end = min(len(lines), i + 6)

                    snippet = CodeSnippet(
                        file_path=file_path,
                        start_line=start + 1,
                        end_line=end,
                        content=''.join(lines[start:end]),
                        relevance_score=len(matches) / len(keywords),
                        metadata={'matched_keywords': matches}
                    )

                    snippets.append(snippet)

            # åˆä½µé‡ç–Šçš„ç‰‡æ®µ
            snippets = self._merge_overlapping_snippets(snippets)

            # æ’åºä¸¦é™åˆ¶æ•¸é‡
            snippets.sort(key=lambda s: s.relevance_score, reverse=True)
            return snippets[:max_snippets]

        except Exception as e:
            console.print(safe_t("context.file.read_error", fallback="[#B565D8]è­¦å‘Šï¼šç„¡æ³•è®€å– {path} - {error}[/#B565D8]").format(path=file_path, error=e))
            return []

    def estimate_token_usage(self, context: Context) -> int:
        """
        ä¼°ç®— Token ä½¿ç”¨é‡

        Args:
            context: ä¸Šä¸‹æ–‡

        Returns:
            int: ä¼°ç®—çš„ token æ•¸
        """
        # ä½¿ç”¨å·²ç¶“è¨ˆç®—å¥½çš„ estimated_tokensï¼Œç¢ºä¿èˆ‡ build_for_task ä¸€è‡´
        total_tokens = sum(fc.estimated_tokens for fc in context.file_contexts)

        return total_tokens

    def compress_context(
        self,
        context: Context,
        target_reduction: float = 0.5
    ) -> Context:
        """
        å£“ç¸®ä¸Šä¸‹æ–‡ï¼ˆç§»é™¤ä½ç›¸é—œæ€§å…§å®¹ï¼‰

        Args:
            context: åŸå§‹ä¸Šä¸‹æ–‡
            target_reduction: ç›®æ¨™æ¸›å°‘æ¯”ä¾‹ï¼ˆ0-1ï¼‰

        Returns:
            Context: å£“ç¸®å¾Œçš„ä¸Šä¸‹æ–‡
        """
        console.print(safe_t("context.compress.starting", fallback="\n[#B565D8]ğŸ—œï¸  å£“ç¸®ä¸Šä¸‹æ–‡...[/#B565D8]"))
        console.print(safe_t("context.compress.original", fallback="  åŸå§‹ tokensï¼š{tokens:,}").format(tokens=context.total_tokens))
        console.print(safe_t("context.compress.target", fallback="  ç›®æ¨™æ¸›å°‘ï¼š{percent:.0f}%").format(percent=target_reduction * 100))

        # ç­–ç•¥ 1ï¼šç§»é™¤ä½ç›¸é—œæ€§æª”æ¡ˆ
        threshold = 0.3
        filtered_files = [
            fc for fc in context.file_contexts
            if fc.relevance_score >= threshold
        ]

        # ç­–ç•¥ 2ï¼šæ¯å€‹æª”æ¡ˆåªä¿ç•™æœ€ç›¸é—œçš„ç‰‡æ®µ
        for fc in filtered_files:
            if len(fc.snippets) > 3:
                fc.snippets = fc.snippets[:3]

        # é‡æ–°è¨ˆç®— token
        compressed_context = Context(
            task_description=context.task_description,
            project_context=context.project_context,
            file_contexts=filtered_files,
            token_budget=context.token_budget,
            metadata=context.metadata
        )

        compressed_context.total_tokens = self.estimate_token_usage(compressed_context)
        compressed_context.included_files = len(filtered_files)

        console.print(safe_t("context.compress.completed", fallback="[#B565D8]âœ“ å£“ç¸®å®Œæˆ[/#B565D8]"))
        console.print(safe_t("context.compress.after", fallback="  å£“ç¸®å¾Œ tokensï¼š{tokens:,}").format(tokens=compressed_context.total_tokens))
        console.print(safe_t("context.compress.actual", fallback="  å¯¦éš›æ¸›å°‘ï¼š{percent:.0f}%").format(percent=(1 - compressed_context.total_tokens / context.total_tokens) * 100))

        return compressed_context

    # ==================== ç§æœ‰æ–¹æ³• ====================

    def _extract_keywords(self, text: str) -> List[str]:
        """å¾æ–‡å­—ä¸­æå–é—œéµå­—"""
        # ç°¡å–®çš„é—œéµå­—æå–ï¼ˆå¯¦éš›æ‡‰ä½¿ç”¨ NLPï¼‰
        # ç§»é™¤å¸¸è¦‹è©å½™
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'please', 'can', 'could', 'should', 'would', 'will', 'do', 'does',
            'è«‹', 'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'èˆ‡', 'æˆ–', 'ä½†', 'ç‚º', 'ä»¥',
            'å¯¦ä½œ', 'æ–°å¢', 'å»ºç«‹', 'ä¿®æ”¹', 'åˆªé™¤', 'åŠŸèƒ½'
        }

        # æå–å–®è©ï¼ˆæ”¯æ´è‹±æ–‡å’Œä¸­æ–‡ï¼‰
        # è‹±æ–‡å–®è©
        english_words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        # ä¸­æ–‡è©å½™ï¼ˆ2-4å€‹å­—å…ƒï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', text)

        all_words = english_words + chinese_words

        # éæ¿¾åœç”¨è©å’ŒçŸ­è©
        keywords = [
            w for w in all_words
            if len(w) > 1 and w.lower() not in stop_words
        ]

        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        return list(dict.fromkeys(keywords))[:20]

    def _calculate_file_relevance(
        self,
        file_path: str,
        task_description: str,
        keywords: List[str]
    ) -> float:
        """
        è¨ˆç®—æª”æ¡ˆç›¸é—œæ€§åˆ†æ•¸ï¼ˆ0-1ï¼‰

        è©•åˆ†å› ç´ ï¼š
        1. æª”æ¡ˆåç¨±åŒ¹é…ï¼ˆ20%ï¼‰
        2. é—œéµå­—å‡ºç¾é »ç‡ï¼ˆ50%ï¼‰
        3. æª”æ¡ˆå¤§å°ï¼ˆé©ä¸­å„ªå…ˆï¼‰ï¼ˆ10%ï¼‰
        4. æœ€è¿‘ä¿®æ”¹æ™‚é–“ï¼ˆ10%ï¼‰
        5. æª”æ¡ˆé¡å‹ï¼ˆ10%ï¼‰
        """
        score = 0.0

        # å› ç´  1ï¼šæª”æ¡ˆåç¨±åŒ¹é…
        file_name = os.path.basename(file_path).lower()
        name_matches = sum(1 for kw in keywords if kw.lower() in file_name)
        score += (name_matches / max(len(keywords), 1)) * 0.2

        # å› ç´  2ï¼šé—œéµå­—å‡ºç¾é »ç‡
        full_path = os.path.join(self.project_path, file_path)
        if os.path.exists(full_path):
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read().lower()

                keyword_count = sum(content.count(kw.lower()) for kw in keywords)
                # æ­£è¦åŒ–ï¼ˆé¿å…éå¤§ï¼‰
                keyword_score = min(keyword_count / 10, 1.0)
                score += keyword_score * 0.5

                # å› ç´  3ï¼šæª”æ¡ˆå¤§å°ï¼ˆé©ä¸­å„ªå…ˆï¼Œé¿å…éå¤§æˆ–éå°ï¼‰
                size = len(content)
                if 500 < size < 10000:  # ç†æƒ³å¤§å°
                    score += 0.1
                elif 100 < size < 50000:  # å¯æ¥å—
                    score += 0.05

            except Exception:
                pass

        # å› ç´  4ï¼šæª”æ¡ˆé¡å‹ï¼ˆå„ªå…ˆ .py ç­‰æºç¢¼æª”æ¡ˆï¼‰
        if file_path.endswith(('.py', '.js', '.ts', '.java')):
            score += 0.1

        return min(score, 1.0)

    def _get_relevance_level(self, score: float) -> RelevanceLevel:
        """å–å¾—ç›¸é—œæ€§ç­‰ç´š"""
        if score >= 0.8:
            return RelevanceLevel.CRITICAL
        elif score >= 0.6:
            return RelevanceLevel.HIGH
        elif score >= 0.4:
            return RelevanceLevel.MEDIUM
        elif score >= 0.2:
            return RelevanceLevel.LOW
        else:
            return RelevanceLevel.MINIMAL

    def _build_file_contexts(
        self,
        files: List[str],
        keywords: List[str],
        token_budget: int
    ) -> List[FileContext]:
        """å»ºç«‹æª”æ¡ˆä¸Šä¸‹æ–‡ï¼ˆæ¼¸é€²å¼è¼‰å…¥ï¼‰"""
        file_contexts = []
        used_tokens = 0

        for file_path in files:
            # æª¢æŸ¥é ç®—
            if used_tokens >= token_budget:
                console.print(safe_t("context.load.budget_reached", fallback="[#B565D8]å·²é” token é ç®—ä¸Šé™ï¼Œåœæ­¢è¼‰å…¥[/#B565D8]"))
                break

            # æå–ç›¸é—œç‰‡æ®µ
            snippets = self.extract_relevant_code(file_path, keywords)

            # å¦‚æœæ²’æœ‰åŒ¹é…çš„ç‰‡æ®µï¼Œè‡³å°‘åŒ…å«æª”æ¡ˆçš„é–‹é ­éƒ¨åˆ†
            if not snippets:
                full_path = os.path.join(self.project_path, file_path)
                if os.path.exists(full_path):
                    try:
                        with open(full_path, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            # å–å‰ 50 è¡Œä½œç‚ºç‰‡æ®µ
                            preview_lines = min(50, len(lines))
                            snippet = CodeSnippet(
                                file_path=file_path,
                                start_line=1,
                                end_line=preview_lines,
                                content=''.join(lines[:preview_lines]),
                                relevance_score=0.1,  # ä½ç›¸é—œæ€§ï¼Œä½†è‡³å°‘æœ‰å…§å®¹
                                metadata={'type': 'file_preview'}
                            )
                            snippets = [snippet]
                    except Exception as e:
                        console.print(safe_t("context.file.read_error", fallback="[#B565D8]è­¦å‘Šï¼šç„¡æ³•è®€å– {path} - {error}[/#B565D8]").format(path=file_path, error=e))
                        continue
                else:
                    continue

            # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
            relevance_score = self._calculate_file_relevance(
                file_path,
                "",  # é€™è£¡ä¸å†éœ€è¦ task_description
                keywords
            )

            # ä¼°ç®— tokens
            estimated_tokens = sum(
                len(s.content) // self.CHARS_PER_TOKEN
                for s in snippets
            )

            # å»ºç«‹æª”æ¡ˆä¸Šä¸‹æ–‡
            file_context = FileContext(
                file_path=file_path,
                relevance_score=relevance_score,
                snippets=snippets,
                estimated_tokens=estimated_tokens
            )

            file_contexts.append(file_context)
            used_tokens += estimated_tokens

        return file_contexts

    def _merge_overlapping_snippets(
        self,
        snippets: List[CodeSnippet]
    ) -> List[CodeSnippet]:
        """åˆä½µé‡ç–Šçš„ç¨‹å¼ç¢¼ç‰‡æ®µ"""
        if not snippets:
            return []

        # æŒ‰èµ·å§‹è¡Œæ’åº
        sorted_snippets = sorted(snippets, key=lambda s: s.start_line)

        merged = [sorted_snippets[0]]

        for current in sorted_snippets[1:]:
            last = merged[-1]

            # å¦‚æœé‡ç–Šæˆ–ç›¸é„°ï¼Œåˆä½µ
            if current.start_line <= last.end_line + 1:
                # åˆä½µ
                last.end_line = max(last.end_line, current.end_line)
                last.content = self._merge_content(
                    last.file_path,
                    last.start_line,
                    last.end_line
                )
                last.relevance_score = max(last.relevance_score, current.relevance_score)
            else:
                merged.append(current)

        return merged

    def _merge_content(
        self,
        file_path: str,
        start_line: int,
        end_line: int
    ) -> str:
        """é‡æ–°è®€å–åˆä½µå¾Œçš„å…§å®¹"""
        full_path = os.path.join(self.project_path, file_path)

        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            return ''.join(lines[start_line - 1:end_line])
        except Exception:
            return ""


def main():
    """æ¸¬è©¦ç”¨ä¸»ç¨‹å¼"""
    import sys

    if len(sys.argv) < 2:
        console.print(safe_t("context.usage.title", fallback="[#B565D8]ç”¨æ³•ï¼š[/#B565D8]"))
        console.print(safe_t("context.usage.syntax", fallback='  python builder.py <å°ˆæ¡ˆè·¯å¾‘> "<ä»»å‹™æè¿°>"'))
        console.print(safe_t("context.usage.example_title", fallback="\n[#B565D8]ç¯„ä¾‹ï¼š[/#B565D8]"))
        console.print(safe_t("context.usage.example", fallback='  python builder.py . "æ–°å¢ä½¿ç”¨è€…ç™»å…¥åŠŸèƒ½"'))
        sys.exit(1)

    project_path = sys.argv[1]
    task_description = sys.argv[2] if len(sys.argv) > 2 else "æ¸¬è©¦ä»»å‹™"

    try:
        builder = ContextBuilder(project_path, token_budget=50000)
        context = builder.build_for_task(task_description, max_files=10)

        console.print(safe_t("context.main.success", fallback="\n[bold green]âœ… ä¸Šä¸‹æ–‡å»ºç«‹æˆåŠŸ[/bold green]"))
        console.print(safe_t("context.main.summary", fallback="\n[#B565D8]ä¸Šä¸‹æ–‡æ‘˜è¦ï¼š[/#B565D8]"))
        console.print(safe_t("context.main.task", fallback="  ä»»å‹™ï¼š{task}").format(task=context.task_description))
        console.print(safe_t("context.main.files", fallback="  æª”æ¡ˆæ•¸ï¼š{count}").format(count=context.included_files))
        console.print(safe_t("context.build.tokens", fallback="  é ä¼° tokensï¼š{tokens:,}").format(tokens=context.total_tokens))

        # é¡¯ç¤ºæª”æ¡ˆåˆ—è¡¨
        console.print(safe_t("context.main.files_list", fallback="\n[#B565D8]åŒ…å«çš„æª”æ¡ˆï¼š[/#B565D8]"))
        for fc in context.file_contexts[:5]:
            console.print(safe_t("context.main.file_snippet", fallback="  - {path} (åˆ†æ•¸: {score:.2f}, ç‰‡æ®µ: {count})").format(path=fc.file_path, score=fc.relevance_score, count=len(fc.snippets)))

    except Exception as e:
        console.print(safe_t("context.main.error", fallback="\n[dim #B565D8]éŒ¯èª¤ï¼š{error}[/red]").format(error=e))
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
