#!/usr/bin/env python3
"""
Gemini è‡ªå‹•å¿«å–ç®¡ç†å™¨
å¾ gemini_chat.py æŠ½é›¢
"""

import time
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)



class AutoCacheManager:
    """è‡ªå‹•å¿«å–ç®¡ç†å™¨"""

    def __init__(self, enabled: bool = False, mode: str = 'auto', threshold: int = 5000, ttl: int = 1):
        self.enabled = enabled
        self.mode = mode  # 'auto' æˆ– 'prompt'
        self.threshold = threshold
        self.ttl_hours = ttl
        self.conversation_pairs = []  # [(user_msg, ai_msg, input_tokens), ...]
        self.total_input_tokens = 0
        self.cache_created = False
        self.active_cache = None
        self.exclude_next = False  # ä¸‹ä¸€æ¬¡å°è©±æ˜¯å¦æ’é™¤

    def add_conversation(self, user_msg: str, ai_msg: str, input_tokens: int):
        """è¨˜éŒ„å°è©±ï¼ˆé™¤éè¢«æ’é™¤ï¼‰"""
        if not self.exclude_next:
            self.conversation_pairs.append((user_msg, ai_msg, input_tokens))
            self.total_input_tokens += input_tokens
        self.exclude_next = False  # é‡ç½®æ’é™¤æ¨™è¨˜

    def should_trigger(self) -> bool:
        """æ˜¯å¦æ‡‰è©²è§¸ç™¼å¿«å–å»ºç«‹"""
        return (self.enabled and
                not self.cache_created and
                self.total_input_tokens >= self.threshold)

    def show_trigger_prompt(self, model_name: str) -> bool:
        """é¡¯ç¤ºå¿«å–è§¸ç™¼æç¤ºï¼ˆå«ç²¾ç¢ºæˆæœ¬è¨ˆç®—ï¼‰"""
        width = console.width - 4  # æ¸›å»é‚Šè·
        print("\n" + "ğŸ”” " + "â”" * (width - 2))
        print("å¿«å–è§¸ç™¼æé†’")
        print("â”" * width)
        print(f"ğŸ“Š ç›®å‰ç‹€æ…‹ï¼š")
        print(f"  ç´¯ç©è¼¸å…¥ï¼š{self.total_input_tokens:,} tokens")
        print(f"  å°è©±è¼ªæ¬¡ï¼š{len(self.conversation_pairs)} æ¬¡")
        print()

        # è¨ˆç®—å¿«å–æœ¬èº«çš„æˆæœ¬èˆ‡ç¯€çœ
        if PRICING_ENABLED:
            try:
                # 1. å¿«å–å»ºç«‹æˆæœ¬ï¼ˆä¸€æ¬¡æ€§ï¼‰
                cache_create_cost, _ = global_pricing_calculator.calculate_text_cost(
                    model_name, self.total_input_tokens, 0, 0
                )

                # 2. æœªä¾†ä½¿ç”¨å¿«å–çš„æˆæœ¬å°æ¯”
                # å‡è¨­å¾ŒçºŒé‚„æœƒè¼¸å…¥ç›¸åŒæ•¸é‡çš„ tokens
                future_input = self.total_input_tokens
                future_output = 2000  # å‡è¨­å¹³å‡è¼¸å‡º

                # ä¸ä½¿ç”¨å¿«å–ï¼šæ¯æ¬¡éƒ½è¦ä»˜å…¨é¡
                no_cache_cost, _ = global_pricing_calculator.calculate_text_cost(
                    model_name, future_input, future_output, 0
                )

                # ä½¿ç”¨å¿«å–ï¼šè¼¸å…¥éƒ¨åˆ†äº« 90% æŠ˜æ‰£
                cache_input_cost, _ = global_pricing_calculator.calculate_text_cost(
                    model_name, future_input, 0, 0
                )
                cache_output_cost, _ = global_pricing_calculator.calculate_text_cost(
                    model_name, 0, future_output, 0
                )
                with_cache_cost = (cache_input_cost * 0.1) + cache_output_cost

                # æ¯æ¬¡ç¯€çœ
                per_query_savings = no_cache_cost - with_cache_cost

                # è¨ˆç®—æç›Šå¹³è¡¡é»ï¼ˆéœ€è¦å¹¾æ¬¡æŸ¥è©¢æ‰å›æœ¬ï¼‰
                if per_query_savings > 0:
                    breakeven = int(cache_create_cost / per_query_savings) + 1
                else:
                    breakeven = 999

                print(f"ğŸ’° æˆæœ¬åˆ†æï¼š")
                print(f"  å¿«å–å»ºç«‹æˆæœ¬ï¼šNT$ {cache_create_cost * USD_TO_TWD:.2f} (ä¸€æ¬¡æ€§)")
                print()
                print(f"  å¾ŒçºŒæ¯æ¬¡æŸ¥è©¢ï¼ˆ{future_input:,} tokens è¼¸å…¥ï¼‰ï¼š")
                print(f"    ä¸ä½¿ç”¨å¿«å–ï¼šNT$ {no_cache_cost * USD_TO_TWD:.2f}")
                print(f"    ä½¿ç”¨å¿«å–ï¼š  NT$ {with_cache_cost * USD_TO_TWD:.2f}")
                print(f"    æ¯æ¬¡ç¯€çœï¼š  NT$ {per_query_savings * USD_TO_TWD:.2f} (çœ {((per_query_savings/no_cache_cost)*100):.0f}%)")
                print()
                print(f"  ğŸ’¡ æç›Šå¹³è¡¡ï¼š{breakeven} æ¬¡æŸ¥è©¢å¾Œé–‹å§‹çœéŒ¢")
                print(f"     (å¿«å–æœ‰æ•ˆæœŸ {self.ttl_hours} å°æ™‚)")
                print()

            except Exception as e:
                logger.warning(f"æˆæœ¬è¨ˆç®—å¤±æ•—: {e}")

        # é¡¯ç¤ºå¿«å–å…§å®¹é è¦½
        print(f"ğŸ“¦ å¿«å–å…§å®¹é è¦½ï¼š")
        preview_count = min(3, len(self.conversation_pairs))
        for i in range(preview_count):
            user_msg, ai_msg, _ = self.conversation_pairs[i]
            user_preview = user_msg[:50] + "..." if len(user_msg) > 50 else user_msg
            ai_preview = ai_msg[:50] + "..." if len(ai_msg) > 50 else ai_msg
            print(f"  - ä½ : {user_preview}")
            print(f"  - AI: {ai_preview}")

        if len(self.conversation_pairs) > preview_count:
            print(f"  ... (å…± {len(self.conversation_pairs)} è¼ªå°è©±)")

        print("â”" * (console.width - 4))
        response = input("å»ºç«‹å¿«å–ï¼Ÿ (y/n) [y]: ").strip().lower()
        return response != 'n'

    def create_cache(self, model_name: str) -> bool:
        """å»ºç«‹å¿«å–"""
        if not CACHE_ENABLED:
            print("âš ï¸  å¿«å–åŠŸèƒ½æœªå•Ÿç”¨ï¼ˆgemini_cache_manager.py æœªæ‰¾åˆ°ï¼‰")
            return False

        try:
            # çµ„åˆå°è©±æ­·å² - å„ªåŒ–ï¼šä½¿ç”¨ list ç´¯ç©ï¼Œå–®æ¬¡ joinï¼ˆé¿å… O(nÂ²) è¨˜æ†¶é«”ä½¿ç”¨ï¼‰
            cache_lines = []
            for user_msg, ai_msg, _ in self.conversation_pairs:
                cache_lines.append("User: ")
                cache_lines.append(user_msg)
                cache_lines.append("\n\nAssistant: ")
                cache_lines.append(ai_msg)
                cache_lines.append("\n\n")

            # å–®æ¬¡åˆ†é…å’Œæ‹·è² - O(n) è¨˜æ†¶é«”è¤‡é›œåº¦
            combined_content = "".join(cache_lines)

            # å»ºç«‹å¿«å–
            print("\nâ³ å»ºç«‹å¿«å–ä¸­...")
            cache = global_cache_manager.create_cache(
                model=model_name,
                contents=[combined_content],
                display_name=f"auto_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                ttl_hours=self.ttl_hours
            )

            self.active_cache = cache
            self.cache_created = True
            print("âœ… å¿«å–å»ºç«‹æˆåŠŸï¼å¾ŒçºŒå°è©±å°‡è‡ªå‹•ä½¿ç”¨å¿«å–ç¯€çœæˆæœ¬ã€‚\n")
            return True

        except Exception as e:
            print(f"âš ï¸  å¿«å–å»ºç«‹å¤±æ•—ï¼š{e}")
            return False


def parse_cache_control(user_input: str, cache_mgr: 'AutoCacheManager') -> tuple:
    """
    è§£æå¿«å–å³æ™‚æ§åˆ¶æŒ‡ä»¤

    Returns:
        (è™•ç†å¾Œçš„è¼¸å…¥, å¿«å–å‹•ä½œ)
    """
    import re

    # [cache:now] - ç«‹å³å»ºç«‹å¿«å–
    if re.search(r'\[cache:now\]', user_input, re.I):
        user_input = re.sub(r'\[cache:now\]', '', user_input, flags=re.I).strip()
        return user_input, 'create_now'

    # [cache:off] - æš«åœè‡ªå‹•å¿«å–
    if re.search(r'\[cache:off\]', user_input, re.I):
        user_input = re.sub(r'\[cache:off\]', '', user_input, flags=re.I).strip()
        cache_mgr.enabled = False
        print("âš ï¸  è‡ªå‹•å¿«å–å·²æš«åœ")
        return user_input, None

    # [cache:on] - æ¢å¾©è‡ªå‹•å¿«å–
    if re.search(r'\[cache:on\]', user_input, re.I):
        user_input = re.sub(r'\[cache:on\]', '', user_input, flags=re.I).strip()
        cache_mgr.enabled = True
        print("âœ“ è‡ªå‹•å¿«å–å·²æ¢å¾©")
        return user_input, None

    # [no-cache] - æœ¬æ¬¡å°è©±ä¸åˆ—å…¥å¿«å–
    if re.search(r'\[no-cache\]', user_input, re.I):
        user_input = re.sub(r'\[no-cache\]', '', user_input, flags=re.I).strip()
        cache_mgr.exclude_next = True
        print("âš ï¸  æœ¬æ¬¡å°è©±ä¸åˆ—å…¥å¿«å–")
        return user_input, None

    return user_input, None
