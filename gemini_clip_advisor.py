#!/usr/bin/env python3
"""
Gemini AI é©…å‹•çš„å‰ªè¼¯å»ºè­°æ¨¡çµ„
ä½¿ç”¨ AI åˆ†æå½±ç‰‡å…§å®¹,æä¾›æ™ºèƒ½å‰ªè¼¯å»ºè­°å’Œç·¨è¼¯æ¨è–¦
"""
import os
import json
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

# å°å…¥ç¾æœ‰æ¨¡çµ„
from gemini_video_preprocessor import VideoPreprocessor
from gemini_scene_detector import SceneDetector, Scene
from utils.api_client import get_gemini_client
from google.genai import types

# å°å…¥åƒ¹æ ¼æ¨¡çµ„
from utils.pricing_loader import get_pricing_calculator, PRICING_ENABLED
from gemini_pricing import USD_TO_TWD
from utils.i18n import safe_t

console = Console()
client = get_gemini_client()

# åˆå§‹åŒ–åƒ¹æ ¼è¨ˆç®—å™¨
global_pricing_calculator = get_pricing_calculator(silent=True)


@dataclass
class ClipSuggestion:
    """å‰ªè¼¯å»ºè­°è³‡æ–™çµæ§‹"""
    id: int
    start_time: float          # é–‹å§‹æ™‚é–“ï¼ˆç§’ï¼‰
    end_time: float            # çµæŸæ™‚é–“ï¼ˆç§’ï¼‰
    duration: float            # æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
    clip_type: str             # é¡å‹ï¼š"highlight", "transition", "key_moment", "intro", "outro"
    description: str           # ç‰‡æ®µæè¿°
    reasoning: str             # æ¨è–¦ç†ç”±
    confidence: float          # ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
    engagement_score: float    # åƒèˆ‡åº¦è©•åˆ†ï¼ˆ0-10ï¼‰
    tags: List[str]            # æ¨™ç±¤åˆ—è¡¨
    frame_preview: str         # é è¦½å¹€è·¯å¾‘
    editing_tips: List[str]    # ç·¨è¼¯å»ºè­°


class ClipAdvisor:
    """AI å‰ªè¼¯å»ºè­°å™¨"""

    def __init__(
        self,
        model_name: str = "gemini-2.0-flash-exp",
        use_scene_detection: bool = True
    ):
        """
        åˆå§‹åŒ–å‰ªè¼¯å»ºè­°å™¨

        Args:
            model_name: ä½¿ç”¨çš„ Gemini æ¨¡å‹
            use_scene_detection: æ˜¯å¦ä½¿ç”¨å ´æ™¯æª¢æ¸¬çµæœ
        """
        self.model_name = model_name
        self.use_scene_detection = use_scene_detection
        self.preprocessor = VideoPreprocessor()
        self.scene_detector = SceneDetector(model_name=model_name) if use_scene_detection else None

    def analyze_and_suggest(
        self,
        video_path: str,
        target_duration: Optional[float] = None,
        clip_types: Optional[List[str]] = None,
        num_suggestions: int = 10
    ) -> List[ClipSuggestion]:
        """
        åˆ†æå½±ç‰‡ä¸¦ç”Ÿæˆå‰ªè¼¯å»ºè­°

        Args:
            video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘
            target_duration: ç›®æ¨™ç¸½é•·åº¦ï¼ˆç§’ï¼‰,None è¡¨ç¤ºä¸é™åˆ¶
            clip_types: è¦ç”Ÿæˆçš„ç‰‡æ®µé¡å‹åˆ—è¡¨,None è¡¨ç¤ºå…¨éƒ¨
            num_suggestions: å»ºè­°æ•¸é‡

        Returns:
            å‰ªè¼¯å»ºè­°åˆ—è¡¨
        """
        console.print(safe_t('media.clip.analysis_title', fallback='\n[bold #E8C4F0]ğŸ¬ AI å‰ªè¼¯å»ºè­°åˆ†æ[/bold #E8C4F0]\n'))
        console.print(safe_t('media.clip.video_file', fallback='ğŸ“ å½±ç‰‡ï¼š{name}', name=os.path.basename(video_path)))

        # 1. ç²å–å½±ç‰‡è³‡è¨Š
        info = self.preprocessor.get_video_info(video_path)
        if not info:
            console.print(safe_t('error.video_info_failed', fallback='[dim #E8C4F0]éŒ¯èª¤ï¼šç„¡æ³•ç²å–å½±ç‰‡è³‡è¨Š[/red]'))
            return []

        duration = info['duration']
        console.print(safe_t('media.clip.total_duration', fallback='â±ï¸  ç¸½é•·åº¦ï¼š{time}', time=self._format_time(duration)))

        if target_duration:
            console.print(safe_t('media.clip.target_duration', fallback='ğŸ¯ ç›®æ¨™é•·åº¦ï¼š{time}', time=self._format_time(target_duration)))

        # 2. å ´æ™¯æª¢æ¸¬ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        scenes = []
        if self.use_scene_detection:
            console.print(safe_t('media.clip.scene_detection', fallback='\n[#E8C4F0]ğŸ“¦ åŸ·è¡Œå ´æ™¯æª¢æ¸¬...[/#E8C4F0]'))
            scenes = self.scene_detector.detect_scenes(video_path, num_keyframes=20)
            console.print(safe_t('media.clip.scenes_found', fallback='âœ“ æª¢æ¸¬åˆ° {count} å€‹å ´æ™¯', count=len(scenes)))

        # 3. åˆ†æå…§å®¹ç‰¹å¾µ
        console.print(safe_t('media.clip.analyzing_features', fallback='\n[#E8C4F0]ğŸ” åˆ†æå½±ç‰‡å…§å®¹ç‰¹å¾µ...[/#E8C4F0]'))
        content_features = self._analyze_content_features(video_path, scenes, duration)

        # 4. ç”Ÿæˆå‰ªè¼¯å»ºè­°
        console.print(safe_t('media.clip.generating_suggestions', fallback='\n[#E8C4F0]ğŸ’¡ ç”Ÿæˆå‰ªè¼¯å»ºè­°...[/#E8C4F0]'))
        suggestions = self._generate_suggestions(
            video_path,
            scenes,
            content_features,
            duration,
            target_duration,
            clip_types,
            num_suggestions
        )

        # 5. æ’åºä¸¦ç¯©é¸
        suggestions = self._rank_and_filter_suggestions(suggestions, num_suggestions)

        console.print(safe_t('media.clip.suggestions_generated', fallback='\n[#B565D8]âœ“ å·²ç”Ÿæˆ {count} å€‹å‰ªè¼¯å»ºè­°[/green]', count=len(suggestions)))

        return suggestions

    def _analyze_content_features(
        self,
        video_path: str,
        scenes: List[Scene],
        duration: float
    ) -> Dict:
        """åˆ†æå½±ç‰‡å…§å®¹ç‰¹å¾µ"""
        features = {
            'has_scenes': len(scenes) > 0,
            'scene_count': len(scenes),
            'avg_scene_duration': 0,
            'scene_variety': 0,
            'key_moments': [],
            'pacing': 'unknown'
        }

        if scenes:
            # è¨ˆç®—å¹³å‡å ´æ™¯é•·åº¦
            scene_durations = [s.end_time - s.start_time for s in scenes]
            features['avg_scene_duration'] = sum(scene_durations) / len(scene_durations)

            # è©•ä¼°å ´æ™¯å¤šæ¨£æ€§ï¼ˆåŸºæ–¼æè¿°å·®ç•°ï¼‰
            unique_keywords = set()
            for scene in scenes:
                unique_keywords.update(scene.key_elements)
            features['scene_variety'] = len(unique_keywords)

            # è­˜åˆ¥é—œéµæ™‚åˆ»ï¼ˆé«˜ç½®ä¿¡åº¦æˆ–é•·å ´æ™¯ï¼‰
            for scene in scenes:
                scene_duration = scene.end_time - scene.start_time
                if scene.confidence > 0.8 or scene_duration > features['avg_scene_duration'] * 1.5:
                    features['key_moments'].append({
                        'time': scene.start_time,
                        'description': scene.description,
                        'duration': scene_duration
                    })

            # è©•ä¼°ç¯€å¥ï¼ˆå¿«é€Ÿ/ä¸­ç­‰/ç·©æ…¢ï¼‰
            if features['avg_scene_duration'] < 5:
                features['pacing'] = 'fast'
            elif features['avg_scene_duration'] < 10:
                features['pacing'] = 'medium'
            else:
                features['pacing'] = 'slow'

        return features

    def _generate_suggestions(
        self,
        video_path: str,
        scenes: List[Scene],
        features: Dict,
        duration: float,
        target_duration: Optional[float],
        clip_types: Optional[List[str]],
        num_suggestions: int
    ) -> List[ClipSuggestion]:
        """ç”Ÿæˆå‰ªè¼¯å»ºè­°"""
        suggestions = []
        suggestion_id = 1

        # å®šç¾©ç‰‡æ®µé¡å‹ç­–ç•¥
        all_clip_types = ['highlight', 'key_moment', 'transition', 'intro', 'outro']
        requested_types = clip_types if clip_types else all_clip_types

        # 1. é–‹å ´å»ºè­°ï¼ˆå¦‚æœè«‹æ±‚ï¼‰
        if 'intro' in requested_types and duration > 10:
            suggestions.append(self._suggest_intro(video_path, scenes, suggestion_id))
            suggestion_id += 1

        # 2. çµå°¾å»ºè­°ï¼ˆå¦‚æœè«‹æ±‚ï¼‰
        if 'outro' in requested_types and duration > 10:
            suggestions.append(self._suggest_outro(video_path, scenes, duration, suggestion_id))
            suggestion_id += 1

        # 3. é—œéµæ™‚åˆ»å»ºè­°
        if 'key_moment' in requested_types:
            for moment in features['key_moments'][:5]:  # æœ€å¤š 5 å€‹
                suggestion = self._suggest_key_moment(
                    video_path,
                    moment,
                    features,
                    suggestion_id
                )
                if suggestion:
                    suggestions.append(suggestion)
                    suggestion_id += 1

        # 4. ç²¾å½©ç‰‡æ®µå»ºè­°
        if 'highlight' in requested_types and scenes:
            highlight_suggestions = self._suggest_highlights(
                video_path,
                scenes,
                features,
                num_suggestions,
                suggestion_id
            )
            suggestions.extend(highlight_suggestions)
            suggestion_id += len(highlight_suggestions)

        # 5. è½‰å ´å»ºè­°
        if 'transition' in requested_types and scenes:
            transition_suggestions = self._suggest_transitions(
                video_path,
                scenes,
                suggestion_id
            )
            suggestions.extend(transition_suggestions[:3])  # æœ€å¤š 3 å€‹

        return suggestions

    def _suggest_intro(
        self,
        video_path: str,
        scenes: List[Scene],
        suggestion_id: int
    ) -> ClipSuggestion:
        """å»ºè­°é–‹å ´ç‰‡æ®µ"""
        # ä½¿ç”¨å‰ 5-10 ç§’ä½œç‚ºé–‹å ´
        start_time = 0
        end_time = min(10.0, scenes[0].end_time if scenes else 10.0)
        duration = end_time - start_time

        # æå–é è¦½å¹€
        frame_path = self._extract_preview_frame(video_path, start_time + duration / 2)

        return ClipSuggestion(
            id=suggestion_id,
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            clip_type="intro",
            description="å½±ç‰‡é–‹å ´ç‰‡æ®µ",
            reasoning="é–‹å ´ç‰‡æ®µå¯ä»¥å¸å¼•è§€çœ¾æ³¨æ„,å»ºç«‹å½±ç‰‡åŸºèª¿",
            confidence=0.9,
            engagement_score=8.0,
            tags=["opening", "intro", "start"],
            frame_preview=frame_path,
            editing_tips=[
                "å¯æ·»åŠ æ¨™é¡Œæ–‡å­—æˆ– Logo",
                "è€ƒæ…®æ·»åŠ èƒŒæ™¯éŸ³æ¨‚æ·¡å…¥æ•ˆæœ",
                "ç¢ºä¿éŸ³é‡å¹³è¡¡"
            ]
        )

    def _suggest_outro(
        self,
        video_path: str,
        scenes: List[Scene],
        duration: float,
        suggestion_id: int
    ) -> ClipSuggestion:
        """å»ºè­°çµå°¾ç‰‡æ®µ"""
        # ä½¿ç”¨æœ€å¾Œ 5-10 ç§’ä½œç‚ºçµå°¾
        end_time = duration
        start_time = max(duration - 10.0, scenes[-1].start_time if scenes else duration - 10.0)
        clip_duration = end_time - start_time

        # æå–é è¦½å¹€
        frame_path = self._extract_preview_frame(video_path, start_time + clip_duration / 2)

        return ClipSuggestion(
            id=suggestion_id,
            start_time=start_time,
            end_time=end_time,
            duration=clip_duration,
            clip_type="outro",
            description="å½±ç‰‡çµå°¾ç‰‡æ®µ",
            reasoning="çµå°¾ç‰‡æ®µå¯ä»¥ç¸½çµå…§å®¹,å¼•å°å¾ŒçºŒè¡Œå‹•",
            confidence=0.9,
            engagement_score=7.5,
            tags=["ending", "outro", "conclusion"],
            frame_preview=frame_path,
            editing_tips=[
                "å¯æ·»åŠ  CTAï¼ˆè¡Œå‹•å‘¼ç±²ï¼‰",
                "è€ƒæ…®æ·»åŠ ç›¸é—œå½±ç‰‡æ¨è–¦",
                "æ·»åŠ éŸ³æ¨‚æ·¡å‡ºæ•ˆæœ"
            ]
        )

    def _suggest_key_moment(
        self,
        video_path: str,
        moment: Dict,
        features: Dict,
        suggestion_id: int
    ) -> Optional[ClipSuggestion]:
        """å»ºè­°é—œéµæ™‚åˆ»ç‰‡æ®µ"""
        start_time = max(0, moment['time'] - 2)  # å‰ 2 ç§’
        end_time = min(moment['time'] + moment['duration'] + 2, moment['time'] + 15)  # æœ€å¤š 15 ç§’
        duration = end_time - start_time

        # æå–é è¦½å¹€
        frame_path = self._extract_preview_frame(video_path, moment['time'])

        # åˆ†æè©²æ™‚åˆ»çš„å…§å®¹
        analysis = self._analyze_moment(video_path, moment['time'], moment['description'])

        return ClipSuggestion(
            id=suggestion_id,
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            clip_type="key_moment",
            description=moment['description'],
            reasoning=f"é—œéµæ™‚åˆ»ï¼š{analysis.get('reasoning', 'é‡è¦å ´æ™¯æˆ–è½‰æŠ˜é»')}",
            confidence=0.85,
            engagement_score=analysis.get('engagement_score', 8.5),
            tags=analysis.get('tags', ['important', 'key']),
            frame_preview=frame_path,
            editing_tips=analysis.get('tips', [
                "è€ƒæ…®æ·»åŠ ç‰¹æ•ˆæˆ–æ…¢å‹•ä½œ",
                "å¯èƒ½é©åˆæ·»åŠ å­—å¹•èªªæ˜",
                "ç¢ºä¿éŸ³è¨Šæ¸…æ™°"
            ])
        )

    def _suggest_highlights(
        self,
        video_path: str,
        scenes: List[Scene],
        features: Dict,
        num_suggestions: int,
        start_id: int
    ) -> List[ClipSuggestion]:
        """å»ºè­°ç²¾å½©ç‰‡æ®µ"""
        suggestions = []

        # æ ¹æ“šå ´æ™¯ç½®ä¿¡åº¦å’Œé•·åº¦é¸æ“‡ç²¾å½©ç‰‡æ®µ
        scored_scenes = []
        for scene in scenes:
            # è·³éå¤ªçŸ­çš„å ´æ™¯
            scene_duration = scene.end_time - scene.start_time
            if scene_duration < 3:
                continue

            # è¨ˆç®—åˆ†æ•¸ï¼ˆç½®ä¿¡åº¦ + é•·åº¦é©ä¸­æ€§ï¼‰
            duration_score = 1.0 if 5 <= scene_duration <= 15 else 0.5
            total_score = scene.confidence * 0.7 + duration_score * 0.3

            scored_scenes.append((total_score, scene))

        # æ’åºä¸¦é¸æ“‡å‰ N å€‹
        scored_scenes.sort(reverse=True, key=lambda x: x[0])
        top_scenes = scored_scenes[:min(num_suggestions, len(scored_scenes))]

        for idx, (score, scene) in enumerate(top_scenes):
            scene_duration = scene.end_time - scene.start_time

            # åˆ†æå ´æ™¯å…§å®¹
            analysis = self._analyze_scene_for_highlight(scene)

            suggestions.append(ClipSuggestion(
                id=start_id + idx,
                start_time=scene.start_time,
                end_time=scene.end_time,
                duration=scene_duration,
                clip_type="highlight",
                description=scene.description,
                reasoning=f"é«˜è³ªé‡å ´æ™¯ï¼š{analysis.get('reasoning', 'è¦–è¦ºæ•ˆæœå¥½,å…§å®¹è±å¯Œ')}",
                confidence=score,
                engagement_score=analysis.get('engagement_score', score * 10),
                tags=scene.key_elements + analysis.get('extra_tags', []),
                frame_preview=scene.start_frame_path,
                editing_tips=analysis.get('tips', [
                    "å¯ä½œç‚ºç¨ç«‹çŸ­ç‰‡ç™¼å¸ƒ",
                    "é©åˆç¤¾ç¾¤åª’é«”åˆ†äº«",
                    "è€ƒæ…®æ·»åŠ èƒŒæ™¯éŸ³æ¨‚"
                ])
            ))

        return suggestions

    def _suggest_transitions(
        self,
        video_path: str,
        scenes: List[Scene],
        start_id: int
    ) -> List[ClipSuggestion]:
        """å»ºè­°è½‰å ´ç‰‡æ®µ"""
        suggestions = []

        # è­˜åˆ¥å ´æ™¯é–“çš„è½‰å ´é»
        for i in range(len(scenes) - 1):
            current_scene = scenes[i]
            next_scene = scenes[i + 1]

            # è½‰å ´å€é–“ï¼šç•¶å‰å ´æ™¯çµæŸå‰ 1 ç§’åˆ°ä¸‹ä¸€å ´æ™¯é–‹å§‹å¾Œ 1 ç§’
            start_time = max(0, current_scene.end_time - 1)
            end_time = min(next_scene.start_time + 1, next_scene.end_time)
            duration = end_time - start_time

            if duration < 0.5:  # è·³éå¤ªçŸ­çš„è½‰å ´
                continue

            # æå–é è¦½å¹€
            frame_path = self._extract_preview_frame(video_path, current_scene.end_time)

            # åˆ†æè½‰å ´é¡å‹
            transition_type = self._analyze_transition_type(current_scene, next_scene)

            suggestions.append(ClipSuggestion(
                id=start_id + i,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                clip_type="transition",
                description=f"å¾ã€Œ{current_scene.description}ã€åˆ°ã€Œ{next_scene.description}ã€",
                reasoning=f"å ´æ™¯è½‰æ›é»ï¼š{transition_type}",
                confidence=0.7,
                engagement_score=6.0,
                tags=["transition", transition_type, "scene_change"],
                frame_preview=frame_path,
                editing_tips=[
                    f"å»ºè­°ä½¿ç”¨{transition_type}è½‰å ´æ•ˆæœ",
                    "ç¢ºä¿éŸ³è¨Šé †æš¢éæ¸¡",
                    "å¯æ·»åŠ è½‰å ´éŸ³æ•ˆ"
                ]
            ))

        return suggestions

    def _analyze_moment(self, video_path: str, timestamp: float, description: str) -> Dict:
        """åˆ†æé—œéµæ™‚åˆ»"""
        # ç°¡åŒ–ç‰ˆï¼šåŸºæ–¼æè¿°çš„é—œéµå­—åˆ†æ
        keywords = description.lower()

        analysis = {
            'reasoning': 'é‡è¦å ´æ™¯æˆ–è½‰æŠ˜é»',
            'engagement_score': 8.0,
            'tags': ['important', 'key'],
            'tips': [
                "è€ƒæ…®æ·»åŠ ç‰¹æ•ˆæˆ–æ…¢å‹•ä½œ",
                "å¯èƒ½é©åˆæ·»åŠ å­—å¹•èªªæ˜",
                "ç¢ºä¿éŸ³è¨Šæ¸…æ™°"
            ]
        }

        # æ ¹æ“šé—œéµå­—èª¿æ•´
        if any(word in keywords for word in ['person', 'people', 'face', 'äººç‰©', 'äºº']):
            analysis['tags'].append('äººç‰©')
            analysis['engagement_score'] += 0.5
            analysis['tips'].append("äººç‰©ç‰¹å¯«å¯æå‡è§€çœ¾é€£çµ")

        if any(word in keywords for word in ['action', 'moving', 'motion', 'å‹•ä½œ', 'é‹å‹•']):
            analysis['tags'].append('å‹•æ…‹')
            analysis['engagement_score'] += 0.5
            analysis['tips'].append("å‹•æ…‹ç•«é¢å¯è€ƒæ…®æ…¢å‹•ä½œæ•ˆæœ")

        if any(word in keywords for word in ['text', 'sign', 'writing', 'æ–‡å­—', 'æ¨™èªŒ']):
            analysis['tags'].append('è³‡è¨Š')
            analysis['reasoning'] = 'åŒ…å«é‡è¦æ–‡å­—æˆ–æ¨™èªŒè³‡è¨Š'

        return analysis

    def _analyze_scene_for_highlight(self, scene: Scene) -> Dict:
        """åˆ†æå ´æ™¯ä½œç‚ºç²¾å½©ç‰‡æ®µçš„æ½›åŠ›"""
        analysis = {
            'reasoning': 'è¦–è¦ºæ•ˆæœå¥½,å…§å®¹è±å¯Œ',
            'engagement_score': scene.confidence * 10,
            'extra_tags': [],
            'tips': [
                "å¯ä½œç‚ºç¨ç«‹çŸ­ç‰‡ç™¼å¸ƒ",
                "é©åˆç¤¾ç¾¤åª’é«”åˆ†äº«"
            ]
        }

        # æ ¹æ“šå ´æ™¯å…ƒç´ èª¿æ•´
        if len(scene.key_elements) > 5:
            analysis['reasoning'] = 'å…§å®¹å…ƒç´ è±å¯Œ,è¦–è¦ºå±¤æ¬¡å¤š'
            analysis['engagement_score'] += 0.5

        # æ ¹æ“šå ´æ™¯é•·åº¦èª¿æ•´
        duration = scene.end_time - scene.start_time
        if 8 <= duration <= 12:
            analysis['extra_tags'].append('optimal_length')
            analysis['tips'].append("é•·åº¦é©ä¸­,é©åˆå„å¹³å°")
        elif duration > 15:
            analysis['tips'].append("å¯è€ƒæ…®å‰ªè¼¯ç‚ºå¤šå€‹ç‰‡æ®µ")

        return analysis

    def _analyze_transition_type(self, scene1: Scene, scene2: Scene) -> str:
        """åˆ†æè½‰å ´é¡å‹"""
        # åŸºæ–¼å ´æ™¯å…ƒç´ çš„ç›¸ä¼¼åº¦åˆ¤æ–·è½‰å ´é¡å‹
        common_elements = set(scene1.key_elements) & set(scene2.key_elements)
        similarity = len(common_elements) / max(len(scene1.key_elements), len(scene2.key_elements), 1)

        if similarity > 0.5:
            return "æ·¡å…¥æ·¡å‡ºï¼ˆå…§å®¹ç›¸é—œï¼‰"
        elif similarity > 0.2:
            return "äº¤å‰æ·¡åŒ–ï¼ˆéƒ¨åˆ†ç›¸é—œï¼‰"
        else:
            return "åˆ‡æ›ï¼ˆå°æ¯”å ´æ™¯ï¼‰"

    def _extract_preview_frame(self, video_path: str, timestamp: float) -> str:
        """æå–é è¦½å¹€"""
        output_dir = Path(video_path).parent / f"{Path(video_path).stem}_clip_previews"
        output_dir.mkdir(exist_ok=True)

        frame_filename = f"preview_{timestamp:.2f}s.jpg"
        frame_path = output_dir / frame_filename

        # ä½¿ç”¨ FFmpeg æå–å¹€
        import subprocess
        cmd = [
            'ffmpeg', '-y',
            '-ss', str(timestamp),
            '-i', video_path,
            '-vframes', '1',
            '-q:v', '2',
            str(frame_path)
        ]

        try:
            subprocess.run(cmd, capture_output=True, check=True)
            return str(frame_path)
        except Exception as e:
            console.print(safe_t('media.clip.preview_frame_warning', fallback='[#E8C4F0]è­¦å‘Šï¼šæå–é è¦½å¹€å¤±æ•— ({time}s): {error}[/#E8C4F0]', time=timestamp, error=e))
            return ""

    def _rank_and_filter_suggestions(
        self,
        suggestions: List[ClipSuggestion],
        max_count: int
    ) -> List[ClipSuggestion]:
        """æ’åºä¸¦ç¯©é¸å»ºè­°"""
        # ç¶œåˆè©•åˆ†ï¼šç½®ä¿¡åº¦ 40% + åƒèˆ‡åº¦è©•åˆ† 60%
        for suggestion in suggestions:
            suggestion.engagement_score = (
                suggestion.confidence * 0.4 * 10 +
                suggestion.engagement_score * 0.6
            )

        # æ’åº
        suggestions.sort(key=lambda x: x.engagement_score, reverse=True)

        # é™åˆ¶æ•¸é‡
        return suggestions[:max_count]

    def display_suggestions(self, suggestions: List[ClipSuggestion]):
        """é¡¯ç¤ºå‰ªè¼¯å»ºè­°"""
        if not suggestions:
            console.print(safe_t('media.clip.no_suggestions', fallback='[#E8C4F0]æ²’æœ‰ç”Ÿæˆå‰ªè¼¯å»ºè­°[/#E8C4F0]'))
            return

        console.print(safe_t('media.clip.suggestions_list', fallback='\n[bold #E8C4F0]ğŸ“‹ å‰ªè¼¯å»ºè­°åˆ—è¡¨ï¼ˆ{count} å€‹ï¼‰[/bold #E8C4F0]\n', count=len(suggestions)))

        # å‰µå»ºè¡¨æ ¼
        table = Table(show_header=True, header_style="bold #B565D8")
        console_width = console.width or 120
        table.add_column("#", style="dim", width=max(4, int(console_width * 0.03)))
        table.add_column("é¡å‹", width=max(10, int(console_width * 0.10)))
        table.add_column("æ™‚é–“ç¯„åœ", width=max(18, int(console_width * 0.15)))
        table.add_column("æè¿°", width=max(35, int(console_width * 0.50)))
        table.add_column("è©•åˆ†", justify="right", width=max(8, int(console_width * 0.08)))

        for suggestion in suggestions:
            # é¡å‹é¡è‰²
            type_colors = {
                'intro': '#E8C4F0',
                'outro': '#E8C4F0',
                'highlight': 'green',
                'key_moment': '#E8C4F0',
                'transition': '#E8C4F0'
            }
            type_color = type_colors.get(suggestion.clip_type, 'white')

            # æ™‚é–“ç¯„åœ
            time_range = f"{self._format_time(suggestion.start_time)} - {self._format_time(suggestion.end_time)}"

            # è©•åˆ†é¡è‰²
            score = suggestion.engagement_score
            if score >= 8:
                score_style = "bold green"
            elif score >= 6:
                score_style = "#E8C4F0"
            else:
                score_style = "dim"

            table.add_row(
                str(suggestion.id),
                f"[{type_color}]{suggestion.clip_type}[/{type_color}]",
                time_range,
                suggestion.description[:40] + "..." if len(suggestion.description) > 40 else suggestion.description,
                f"[{score_style}]{score:.1f}[/{score_style}]"
            )

        console.print(table)

        # é¡¯ç¤ºè©³ç´°è³‡è¨Š
        console.print(safe_t('media.clip.detailed_suggestions', fallback='\n[bold #E8C4F0]ğŸ’¡ è©³ç´°å»ºè­°ï¼š[/bold #E8C4F0]\n'))
        for suggestion in suggestions[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹çš„è©³ç´°è³‡è¨Š
            self._display_suggestion_detail(suggestion)

    def _display_suggestion_detail(self, suggestion: ClipSuggestion):
        """é¡¯ç¤ºå–®å€‹å»ºè­°çš„è©³ç´°è³‡è¨Š"""
        content = f"""
[#E8C4F0]æ™‚é–“ï¼š[/#E8C4F0] {self._format_time(suggestion.start_time)} - {self._format_time(suggestion.end_time)} ({suggestion.duration:.1f}s)
[#E8C4F0]é¡å‹ï¼š[/#E8C4F0] {suggestion.clip_type}
[#E8C4F0]æè¿°ï¼š[/#E8C4F0] {suggestion.description}
[#E8C4F0]æ¨è–¦ç†ç”±ï¼š[/#E8C4F0] {suggestion.reasoning}
[#E8C4F0]è©•åˆ†ï¼š[/#E8C4F0] {suggestion.engagement_score:.1f}/10
[#E8C4F0]æ¨™ç±¤ï¼š[/#E8C4F0] {', '.join(suggestion.tags)}

[#E8C4F0]ç·¨è¼¯å»ºè­°ï¼š[/#E8C4F0]
"""
        for tip in suggestion.editing_tips:
            content += f"  â€¢ {tip}\n"

        console.print(Panel(content, title=f"[bold]#{suggestion.id} - {suggestion.clip_type.upper()}[/bold]", border_style="#B565D8"))
        console.print()

    def save_suggestions(
        self,
        suggestions: List[ClipSuggestion],
        video_path: str,
        format: str = 'json'
    ) -> str:
        """
        ä¿å­˜å‰ªè¼¯å»ºè­°

        Args:
            suggestions: å»ºè­°åˆ—è¡¨
            video_path: å½±ç‰‡è·¯å¾‘
            format: è¼¸å‡ºæ ¼å¼ï¼ˆ'json', 'txt', 'edl'ï¼‰

        Returns:
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        video_name = Path(video_path).stem
        output_dir = Path(video_path).parent
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        if format == 'json':
            output_file = output_dir / f"{video_name}_clip_suggestions_{timestamp}.json"
            data = {
                'video': os.path.basename(video_path),
                'generated_at': datetime.now().isoformat(),
                'total_suggestions': len(suggestions),
                'suggestions': [asdict(s) for s in suggestions]
            }
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

        elif format == 'txt':
            output_file = output_dir / f"{video_name}_clip_suggestions_{timestamp}.txt"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"å½±ç‰‡å‰ªè¼¯å»ºè­° - {os.path.basename(video_path)}\n")
                f.write(f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ç¸½å»ºè­°æ•¸ï¼š{len(suggestions)}\n")
                f.write("=" * 80 + "\n\n")

                for suggestion in suggestions:
                    f.write(f"#{suggestion.id} - {suggestion.clip_type.upper()}\n")
                    f.write(f"æ™‚é–“ï¼š{self._format_time(suggestion.start_time)} - {self._format_time(suggestion.end_time)} ({suggestion.duration:.1f}s)\n")
                    f.write(f"æè¿°ï¼š{suggestion.description}\n")
                    f.write(f"æ¨è–¦ç†ç”±ï¼š{suggestion.reasoning}\n")
                    f.write(f"è©•åˆ†ï¼š{suggestion.engagement_score:.1f}/10\n")
                    f.write(f"æ¨™ç±¤ï¼š{', '.join(suggestion.tags)}\n")
                    f.write("ç·¨è¼¯å»ºè­°ï¼š\n")
                    for tip in suggestion.editing_tips:
                        f.write(f"  â€¢ {tip}\n")
                    f.write("-" * 80 + "\n\n")

        elif format == 'edl':
            output_file = output_dir / f"{video_name}_clip_suggestions_{timestamp}.edl"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("TITLE: Clip Suggestions\n")
                f.write("FCM: NON-DROP FRAME\n\n")

                for idx, suggestion in enumerate(suggestions, 1):
                    # EDL æ ¼å¼ï¼šç·¨è™Ÿ æº é€šé“ é¡å‹ èµ·å§‹ çµæŸ èµ·å§‹ çµæŸ
                    start_tc = self._seconds_to_timecode(suggestion.start_time)
                    end_tc = self._seconds_to_timecode(suggestion.end_time)

                    f.write(f"{idx:03d}  001      V     C        {start_tc} {end_tc} {start_tc} {end_tc}\n")
                    f.write(f"* FROM CLIP NAME: {suggestion.description}\n")
                    f.write(f"* COMMENT: {suggestion.clip_type} - {suggestion.reasoning}\n\n")

        else:
            console.print(safe_t('error.unsupported_format', fallback='[dim #E8C4F0]ä¸æ”¯æ´çš„æ ¼å¼ï¼š{format}[/red]', format=format))
            return ""

        console.print(safe_t('media.clip.suggestions_saved', fallback='[#B565D8]âœ“ å‰ªè¼¯å»ºè­°å·²ä¿å­˜ï¼š{file}[/green]', file=output_file))
        return str(output_file)

    def _format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ™‚é–“ï¼ˆç§’ -> mm:ssï¼‰"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"

    def _seconds_to_timecode(self, seconds: float) -> str:
        """è½‰æ›ç§’æ•¸ç‚ºæ™‚é–“ç¢¼ï¼ˆHH:MM:SS:FFï¼‰"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        frames = int((seconds % 1) * 30)  # å‡è¨­ 30 fps
        return f"{hours:02d}:{minutes:02d}:{secs:02d}:{frames:02d}"


# ==================== CLI ä»‹é¢ ====================

def main():
    """ä¸»ç¨‹å¼"""
    import argparse

    parser = argparse.ArgumentParser(description='Gemini AI å‰ªè¼¯å»ºè­°')
    parser.add_argument('video', help='å½±ç‰‡æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--model', default='gemini-2.0-flash-exp', help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--num', type=int, default=10, help='å»ºè­°æ•¸é‡ï¼ˆé è¨­ 10ï¼‰')
    parser.add_argument('--target-duration', type=float, help='ç›®æ¨™ç¸½é•·åº¦ï¼ˆç§’ï¼‰')
    parser.add_argument('--types', nargs='+', choices=['highlight', 'key_moment', 'transition', 'intro', 'outro'],
                       help='ç‰‡æ®µé¡å‹')
    parser.add_argument('--no-scene-detection', action='store_true', help='åœç”¨å ´æ™¯æª¢æ¸¬')
    parser.add_argument('--output', choices=['json', 'txt', 'edl', 'all'], default='all', help='è¼¸å‡ºæ ¼å¼')

    args = parser.parse_args()

    # æª¢æŸ¥æª”æ¡ˆ
    if not os.path.isfile(args.video):
        console.print(safe_t('error.video_not_found', fallback='[dim #E8C4F0]éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆï¼š{path}[/red]', path=args.video))
        return

    # å‰µå»ºå»ºè­°å™¨
    advisor = ClipAdvisor(
        model_name=args.model,
        use_scene_detection=not args.no_scene_detection
    )

    # åˆ†æä¸¦ç”Ÿæˆå»ºè­°
    suggestions = advisor.analyze_and_suggest(
        args.video,
        target_duration=args.target_duration,
        clip_types=args.types,
        num_suggestions=args.num
    )

    # é¡¯ç¤ºçµæœ
    advisor.display_suggestions(suggestions)

    # ä¿å­˜çµæœ
    if args.output == 'all':
        advisor.save_suggestions(suggestions, args.video, format='json')
        advisor.save_suggestions(suggestions, args.video, format='txt')
        advisor.save_suggestions(suggestions, args.video, format='edl')
    else:
        advisor.save_suggestions(suggestions, args.video, format=args.output)


if __name__ == "__main__":
    main()
