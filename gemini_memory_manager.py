#!/usr/bin/env python3
"""
Gemini Memory Manager - è¨˜æ†¶é«”ç®¡ç†æ¨¡çµ„
=====================================

åŠŸèƒ½ï¼š
1. è¨˜æ†¶é«”æ± ç®¡ç† (MemoryPoolManager)
2. å¤§å‹å°è©±æ­·å²åˆ†é  (ConversationManager)
3. åœ–ç‰‡åˆ†å¡Šè¼‰å…¥ (load_image_chunked)
4. å½±ç‰‡åˆ†æ®µè™•ç† (process_video_chunked)
5. æª”æ¡ˆä¸Šå‚³æ–·é»çºŒå‚³ (ChunkedUploader)
6. å¤šç·šç¨‹è™•ç†æ¡†æ¶ (ParallelProcessor)

Author: Saki-tw
Email: Saki@saki-studio.com.tw
Date: 2025-10-23
"""

import os
import io
import gc
import json
import time
import hashlib
import subprocess
import psutil
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from PIL import Image
from rich.console import Console
from utils.i18n import safe_t
from rich.progress import Progress, TaskID, BarColumn, TextColumn, TimeRemainingColumn
from rich.panel import Panel
from rich import print as rprint

console = Console()


# ============================================================================
# 1. è¨˜æ†¶é«”æ± ç®¡ç†å™¨
# ============================================================================

class MemoryPoolManager:
    """è¨˜æ†¶é«”æ± ç®¡ç†å™¨ - ç›£æ§ä¸¦æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨é‡"""

    def __init__(self, max_memory_mb: int = 2048):
        """
        åˆå§‹åŒ–è¨˜æ†¶é«”æ± ç®¡ç†å™¨

        Args:
            max_memory_mb: æœ€å¤§è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)ï¼Œé è¨­ 2GB
        """
        self.max_memory = max_memory_mb * 1024 * 1024  # è½‰æ›ç‚º bytes
        self.process = psutil.Process()
        self.peak_memory = 0
        start_memory=self.get_current_memory()

    def get_current_memory(self) -> int:
        """å–å¾—ç•¶å‰ç¨‹åºçš„è¨˜æ†¶é«”ä½¿ç”¨é‡ (bytes)"""
        return self.process.memory_info().rss

    def get_current_memory_mb(self) -> float:
        """å–å¾—ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)"""
        return self.get_current_memory() / (1024 * 1024)

    def check_memory_usage(self) -> Tuple[bool, float]:
        """
        æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡æ˜¯å¦è¶…éé™åˆ¶

        Returns:
            (æ˜¯å¦å®‰å…¨, ç•¶å‰ä½¿ç”¨é‡ MB)
        """
        current_mb = self.get_current_memory_mb()
        self.peak_memory = max(self.peak_memory, current_mb)

        is_safe = current_mb < (self.max_memory / (1024 * 1024))
        return is_safe, current_mb

    def force_gc(self):
        """å¼·åˆ¶åƒåœ¾å›æ”¶"""
        gc.collect()

    def get_memory_report(self) -> Dict[str, float]:
        """å–å¾—è¨˜æ†¶é«”ä½¿ç”¨å ±å‘Š"""
        current_mb = self.get_current_memory_mb()
        start_mb = self.start_memory / (1024 * 1024)

        return {
            "current_mb": round(current_mb, 2),
            "peak_mb": round(self.peak_memory, 2),
            "start_mb": round(start_mb, 2),
            "delta_mb": round(current_mb - start_mb, 2),
            "max_limit_mb": round(self.max_memory / (1024 * 1024), 2),
            "usage_percent": round((current_mb / (self.max_memory / (1024 * 1024))) * 100, 2)
        }

    def print_memory_report(self):
        """è¼¸å‡ºè¨˜æ†¶é«”ä½¿ç”¨å ±å‘Š"""
        report = self.get_memory_report()

        console.print(Panel(
            f"""[bold #DDA0DD]è¨˜æ†¶é«”ä½¿ç”¨å ±å‘Š[/bold #DDA0DD]

ç•¶å‰ä½¿ç”¨: [#DDA0DD]{report['current_mb']} MB[/#DDA0DD]
å³°å€¼ä½¿ç”¨: [dim #DDA0DD]{report['peak_mb']} MB[/red]
èµ·å§‹ä½¿ç”¨: [#DA70D6]{report['start_mb']} MB[/green]
å¢é‡ä½¿ç”¨: [#DDA0DD]{report['delta_mb']} MB[/#DDA0DD]
ä½¿ç”¨ç‡: [{'red' if report['usage_percent'] > 80 else 'green'}]{report['usage_percent']}%[/]
è¨˜æ†¶é«”é™åˆ¶: {report['max_limit_mb']} MB""",
            title="ğŸ’¾ Memory Report",
            border_style="#DA70D6"
        ))


# ============================================================================
# 2. å°è©±æ­·å²ç®¡ç†å™¨
# ============================================================================

class ConversationManager:
    """å°è©±æ­·å²ç®¡ç†å™¨ - æ”¯æ´åˆ†é èˆ‡è‡ªå‹•å­˜æª”

    .. deprecated:: v1.0.3
        å»ºè­°ä½¿ç”¨ gemini_conversation.ConversationManagerï¼Œè©²ç‰ˆæœ¬æä¾›æ›´å®Œæ•´çš„åŠŸèƒ½ã€‚
        æ­¤ç‰ˆæœ¬å°‡åœ¨ v2.0 ç§»é™¤ã€‚
    """

    def __init__(
        self,
        max_history: int = 100,
        archive_path: Optional[Path] = None,
        auto_archive: bool = True
    ):
        """
        åˆå§‹åŒ–å°è©±ç®¡ç†å™¨

        Args:
            max_history: è¨˜æ†¶é«”ä¸­æœ€å¤šä¿ç•™çš„å°è©±æ•¸
            archive_path: å­˜æª”è·¯å¾‘ï¼Œé è¨­ç‚º ~/.gemini_conversations/
            auto_archive: æ˜¯å¦è‡ªå‹•å­˜æª”
        """
        self.max_history = max_history
        self.history: List[Dict[str, Any]] = []
        self.archived: List[Dict[str, Any]] = []
        self.auto_archive = auto_archive

        # è¨­å®šå­˜æª”è·¯å¾‘
        if archive_path is None:
            # ä½¿ç”¨çµ±ä¸€å¿«å–ç›®éŒ„
            from utils.path_manager import get_cache_dir
            self.archive_path = get_cache_dir('memory_archive')
        else:
            self.archive_path = Path(archive_path)
            self.archive_path.mkdir(parents=True, exist_ok=True)

    def add_message(self, role: str, content: str, metadata: Optional[Dict] = None):
        """
        æ–°å¢è¨Šæ¯åˆ°å°è©±æ­·å²

        Args:
            role: è§’è‰² (user/model)
            content: è¨Šæ¯å…§å®¹
            metadata: é¡å¤–è³‡è¨Š (å¦‚æ™‚é–“æˆ³ã€token æ•¸ç­‰)
        """
        message = {
            "role": role,
            "content": content,
            "timestamp": time.time(),
            "metadata": metadata or {}
        }

        self.history.append(message)

        # æª¢æŸ¥æ˜¯å¦éœ€è¦å­˜æª”
        if len(self.history) > self.max_history and self.auto_archive:
            self._archive_old_messages()

    def _archive_old_messages(self):
        """å°‡èˆŠè¨Šæ¯å­˜æª”åˆ°ç£ç¢Ÿ"""
        # ä¿ç•™æœ€è¿‘ 50% çš„è¨Šæ¯
        split_point = self.max_history // 2

        to_archive = self.history[:split_point]
        history=self.history[split_point:]

        # å¯«å…¥å­˜æª”
        timestamp = int(time.time())
        archive_file = self.archive_path / f"conversation_{timestamp}.json"

        with open(archive_file, 'w', encoding='utf-8') as f:
            json.dump(to_archive, f, ensure_ascii=False, indent=2)

        console.print(safe_t('common.message', fallback='[dim]ğŸ“ å·²å­˜æª” {len(to_archive)} å‰‡å°è©±åˆ° {archive_file.name}[/dim]', to_archive_count=len(to_archive), name=archive_file.name))

        # å¼·åˆ¶åƒåœ¾å›æ”¶
        gc.collect()

    def get_recent_history(self, count: int = 10) -> List[Dict[str, Any]]:
        """å–å¾—æœ€è¿‘ N å‰‡å°è©±"""
        return self.history[-count:]

    def clear_history(self, save_archive: bool = True):
        """æ¸…é™¤æ‰€æœ‰å°è©±æ­·å²"""
        if save_archive and len(self.history) > 0:
            self._archive_old_messages()
        self.history.clear()
        gc.collect()

    def get_statistics(self) -> Dict[str, Any]:
        """å–å¾—å°è©±çµ±è¨ˆè³‡è¨Š"""
        return {
            "current_count": len(self.history),
            "max_history": self.max_history,
            "archive_count": len(list(self.archive_path.glob("*.json"))),
            "usage_percent": round((len(self.history) / self.max_history) * 100, 2)
        }


# ============================================================================
# 3. åœ–ç‰‡åˆ†å¡Šè¼‰å…¥
# ============================================================================

def load_image_chunked(
    file_path: str,
    max_size: Tuple[int, int] = (1920, 1080),
    quality: int = 85
) -> bytes:
    """
    åˆ†å¡Šè¼‰å…¥åœ–ç‰‡ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º

    Args:
        file_path: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘
        max_size: æœ€å¤§å°ºå¯¸ (å¯¬, é«˜)ï¼Œé è¨­ 1920x1080
        quality: JPEG å“è³ª (1-100)

    Returns:
        åœ–ç‰‡çš„ bytes è³‡æ–™

    Example:
        >>> image_data = load_image_chunked("4k_image.jpg")
        >>> # å¯å®‰å…¨è™•ç† 4K åœ–ç‰‡è€Œä¸æœƒ OOM
    """
    try:
        with Image.open(file_path) as img:
            # å–å¾—åŸå§‹å°ºå¯¸
            original_size = img.size
            original_format = img.format or 'JPEG'

            # ç¸®æ”¾è‡³æœ€å¤§å°ºå¯¸ï¼ˆä¿æŒæ¯”ä¾‹ï¼‰
            img.thumbnail(max_size, Image.Resampling.LANCZOS)

            # è½‰æ›ç‚º RGBï¼ˆå¦‚æœæ˜¯ RGBA æˆ–å…¶ä»–æ ¼å¼ï¼‰
            if img.mode in ('RGBA', 'LA', 'P'):
                background = Image.new('RGB', img.size, (255, 255, 255))
                if img.mode == 'P':
                    img = img.convert('RGBA')
                background.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)
                img = background
            elif img.mode != 'RGB':
                img = img.convert('RGB')

            # è½‰æ›ç‚º bytesï¼ˆä½¿ç”¨ BytesIO é¿å…è¼‰å…¥æ•´å€‹åœ–ç‰‡åˆ°è¨˜æ†¶é«”ï¼‰
            buffer = io.BytesIO()

            # æ ¹æ“šåŸå§‹æ ¼å¼é¸æ“‡è¼¸å‡ºæ ¼å¼
            if original_format in ('JPEG', 'JPG'):
                img.save(buffer, format='JPEG', quality=quality, optimize=True)
            elif original_format == 'PNG':
                img.save(buffer, format='PNG', optimize=True)
            else:
                img.save(buffer, format='JPEG', quality=quality, optimize=True)

            image_bytes = buffer.getvalue()
            buffer.close()

            # è¼¸å‡ºè™•ç†è³‡è¨Š
            new_size = img.size
            reduction = round((1 - len(image_bytes) / os.path.getsize(file_path)) * 100, 2)

            console.print(safe_t('common.processing', fallback='[dim]ğŸ–¼ï¸  åœ–ç‰‡è™•ç†: {original_size} â†’ {new_size}, è¨˜æ†¶é«”æ¸›å°‘ {reduction}%[/dim]', original_size=original_size, new_size=new_size, reduction=reduction))

            return image_bytes

    except Exception as e:
        console.print(safe_t('error.failed', fallback='[dim #DDA0DD]âŒ åœ–ç‰‡è¼‰å…¥å¤±æ•—: {e}[/red]', e=e))
        raise


# ============================================================================
# 4. å½±ç‰‡åˆ†æ®µè™•ç†
# ============================================================================

def get_video_duration(video_path: str) -> float:
    """
    å–å¾—å½±ç‰‡æ™‚é•·ï¼ˆç§’ï¼‰

    Args:
        video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘

    Returns:
        å½±ç‰‡æ™‚é•·ï¼ˆç§’ï¼‰
    """
    try:
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            video_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return float(result.stdout.strip())

    except Exception as e:
        console.print(safe_t('error.cannot_process', fallback='[dim #DDA0DD]âŒ ç„¡æ³•å–å¾—å½±ç‰‡æ™‚é•·: {e}[/red]', e=e))
        return 0.0


def process_video_chunked(
    video_path: str,
    output_path: str,
    chunk_duration: int = 60,
    process_func: Optional[Callable] = None,
    cleanup: bool = True
) -> bool:
    """
    åˆ†æ®µè™•ç†å½±ç‰‡ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º

    Args:
        video_path: è¼¸å…¥å½±ç‰‡è·¯å¾‘
        output_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘
        chunk_duration: æ¯æ®µæ™‚é•·ï¼ˆç§’ï¼‰ï¼Œé è¨­ 60 ç§’
        process_func: è™•ç†å‡½æ•¸ï¼Œæ¥æ”¶ (chunk_path, chunk_index) ä¸¦è¿”å›è™•ç†å¾Œçš„è·¯å¾‘
        cleanup: æ˜¯å¦æ¸…ç†è‡¨æ™‚æª”æ¡ˆ

    Returns:
        æ˜¯å¦æˆåŠŸ

    Example:
        >>> def my_process(chunk_path, idx):
        ...     # å° chunk é€²è¡Œè™•ç†
        ...     return processed_path
        >>> process_video_chunked("long_video.mp4", "output.mp4", process_func=my_process)
    """
    try:
        # å–å¾—å½±ç‰‡ç¸½æ™‚é•·
        duration = get_video_duration(video_path)
        if duration == 0:
            return False

        # è¨ˆç®—åˆ†æ®µæ•¸
        num_chunks = int(duration // chunk_duration) + 1
        temp_dir = Path(output_path).parent / "temp_chunks"
        temp_dir.mkdir(exist_ok=True)

        processed_chunks = []

        with Progress(
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
            console=console
        ) as progress:

            task = progress.add_task(f"ğŸ¬ åˆ†æ®µè™•ç†å½±ç‰‡ ({num_chunks} æ®µ)", total=num_chunks)

            for i in range(num_chunks):
                start_time = i * chunk_duration
                chunk_path = temp_dir / f"chunk_{i:04d}.mp4"

                # åˆ‡å‰²å½±ç‰‡ç‰‡æ®µ
                cmd = [
                    'ffmpeg',
                    '-i', video_path,
                    '-ss', str(start_time),
                    '-t', str(chunk_duration),
                    '-c', 'copy',  # ç„¡æè¤‡è£½
                    '-y',  # è¦†å¯«è¼¸å‡º
                    str(chunk_path)
                ]

                subprocess.run(cmd, capture_output=True, check=True)

                # å¦‚æœæœ‰è™•ç†å‡½æ•¸ï¼Œå‰‡è™•ç†æ­¤ç‰‡æ®µ
                if process_func:
                    processed_path = process_func(str(chunk_path), i)
                    processed_chunks.append(processed_path)
                else:
                    processed_chunks.append(str(chunk_path))

                progress.update(task, advance=1)

                # å¼·åˆ¶åƒåœ¾å›æ”¶
                gc.collect()

        # åˆä½µæ‰€æœ‰ç‰‡æ®µ
        console.print(safe_t('common.message', fallback='[#DDA0DD]ğŸ”— åˆä½µå½±ç‰‡ç‰‡æ®µ...[/#DDA0DD]'))
        _merge_video_chunks(processed_chunks, output_path)

        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if cleanup:
            console.print(safe_t('common.message', fallback='[dim]ğŸ§¹ æ¸…ç†è‡¨æ™‚æª”æ¡ˆ...[/dim]'))
            for chunk in processed_chunks:
                if os.path.exists(chunk):
                    os.remove(chunk)
            if temp_dir.exists():
                temp_dir.rmdir()

        console.print(safe_t('common.completed', fallback='[#DA70D6]âœ… å½±ç‰‡è™•ç†å®Œæˆ: {output_path}[/green]', output_path=output_path))
        return True

    except Exception as e:
        console.print(safe_t('error.failed', fallback='[dim #DDA0DD]âŒ å½±ç‰‡è™•ç†å¤±æ•—: {e}[/red]', e=e))
        return False


def _merge_video_chunks(chunk_paths: List[str], output_path: str):
    """åˆä½µå½±ç‰‡ç‰‡æ®µ"""
    # å»ºç«‹ concat æª”æ¡ˆæ¸…å–®
    concat_file = Path(output_path).parent / "concat_list.txt"

    with open(concat_file, 'w') as f:
        for chunk_path in chunk_paths:
            f.write(f"file '{chunk_path}'\n")

    # ä½¿ç”¨ ffmpeg concat
    cmd = [
        'ffmpeg',
        '-f', 'concat',
        '-safe', '0',
        '-i', str(concat_file),
        '-c', 'copy',
        '-y',
        output_path
    ]

    subprocess.run(cmd, capture_output=True, check=True)

    # æ¸…ç† concat æ¸…å–®
    if concat_file.exists():
        concat_file.unlink()


# ============================================================================
# 5. æª”æ¡ˆä¸Šå‚³æ–·é»çºŒå‚³
# ============================================================================

class ChunkedUploader:
    """æª”æ¡ˆåˆ†å¡Šä¸Šå‚³å™¨ - æ”¯æ´æ–·é»çºŒå‚³"""

    CHUNK_SIZE = 5 * 1024 * 1024  # 5MB per chunk

    def __init__(self, progress_dir: Optional[Path] = None):
        """
        åˆå§‹åŒ–åˆ†å¡Šä¸Šå‚³å™¨

        Args:
            progress_dir: é€²åº¦æª”æ¡ˆå­˜æ”¾ç›®éŒ„
        """
        if progress_dir is None:
            # ä½¿ç”¨çµ±ä¸€å¿«å–ç›®éŒ„
            from utils.path_manager import get_cache_dir
            self.progress_dir = get_cache_dir('upload_progress')
        else:
            self.progress_dir = Path(progress_dir)

        self.progress_dir.mkdir(parents=True, exist_ok=True)

    def _get_progress_file(self, file_path: str) -> Path:
        """å–å¾—é€²åº¦æª”æ¡ˆè·¯å¾‘"""
        file_hash = hashlib.md5(file_path.encode()).hexdigest()
        return self.progress_dir / f"upload_{file_hash}.json"

    def _load_progress(self, file_path: str) -> Dict[str, Any]:
        """è¼‰å…¥ä¸Šå‚³é€²åº¦"""
        progress_file = self._get_progress_file(file_path)

        if progress_file.exists():
            with open(progress_file, 'r') as f:
                return json.load(f)

        return {"uploaded_chunks": [], "total_chunks": 0, "completed": False}

    def _save_progress(self, file_path: str, progress: Dict[str, Any]):
        """å„²å­˜ä¸Šå‚³é€²åº¦"""
        progress_file = self._get_progress_file(file_path)

        with open(progress_file, 'w') as f:
            json.dump(progress, f)

    def _clear_progress(self, file_path: str):
        """æ¸…é™¤ä¸Šå‚³é€²åº¦"""
        progress_file = self._get_progress_file(file_path)
        if progress_file.exists():
            progress_file.unlink()

    def upload_file(
        self,
        file_path: str,
        upload_func: Callable[[bytes, int, int], bool],
        resume: bool = True
    ) -> bool:
        """
        åˆ†å¡Šä¸Šå‚³æª”æ¡ˆ

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            upload_func: ä¸Šå‚³å‡½æ•¸ï¼Œæ¥æ”¶ (chunk_data, chunk_index, total_chunks) ä¸¦è¿”å›æ˜¯å¦æˆåŠŸ
            resume: æ˜¯å¦å•Ÿç”¨æ–·é»çºŒå‚³

        Returns:
            æ˜¯å¦ä¸Šå‚³æˆåŠŸ

        Example:
            >>> def my_upload(data, idx, total):
            ...     # ä¸Šå‚³ chunk åˆ° API
            ...     return True
            >>> uploader = ChunkedUploader()
            >>> uploader.upload_file("large_file.mp4", my_upload)
        """
        try:
            file_size = os.path.getsize(file_path)
            total_chunks = (file_size // self.CHUNK_SIZE) + 1

            # è¼‰å…¥é€²åº¦
            progress = self._load_progress(file_path) if resume else {
                "uploaded_chunks": [],
                "total_chunks": total_chunks,
                "completed": False
            }

            # å¦‚æœå·²å®Œæˆï¼Œç›´æ¥è¿”å›
            if progress.get("completed"):
                console.print(safe_t('common.completed', fallback='[#DA70D6]âœ… æª”æ¡ˆå·²ä¸Šå‚³å®Œæˆï¼ˆä½¿ç”¨å¿«å–ï¼‰[/green]'))
                return True

            uploaded_chunks = set(progress["uploaded_chunks"])

            with Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("â€¢"),
                TextColumn("[#DDA0DD]{task.completed}/{task.total} chunks"),
                TimeRemainingColumn(),
                console=console
            ) as progress_bar:

                task = progress_bar.add_task(
                    f"ğŸ“¤ ä¸Šå‚³ {Path(file_path).name}",
                    total=total_chunks
                )

                # è¨­å®šåˆå§‹é€²åº¦
                progress_bar.update(task, completed=len(uploaded_chunks))

                with open(file_path, 'rb') as f:
                    for chunk_idx in range(total_chunks):
                        # è·³éå·²ä¸Šå‚³çš„ chunk
                        if chunk_idx in uploaded_chunks:
                            continue

                        # è®€å– chunk
                        f.seek(chunk_idx * self.CHUNK_SIZE)
                        chunk_data = f.read(self.CHUNK_SIZE)

                        # ä¸Šå‚³ chunk
                        success = upload_func(chunk_data, chunk_idx, total_chunks)

                        if not success:
                            console.print(safe_t('error.failed', fallback='[dim #DDA0DD]âŒ Chunk {chunk_idx} ä¸Šå‚³å¤±æ•—[/red]', chunk_idx=chunk_idx))
                            return False

                        # æ›´æ–°é€²åº¦
                        uploaded_chunks.add(chunk_idx)
                        progress["uploaded_chunks"] = list(uploaded_chunks)
                        self._save_progress(file_path, progress)

                        progress_bar.update(task, advance=1)

                        # é‡‹æ”¾è¨˜æ†¶é«”
                        del chunk_data
                        gc.collect()

            # æ¨™è¨˜ç‚ºå®Œæˆ
            progress["completed"] = True
            self._save_progress(file_path, progress)

            console.print(safe_t('common.completed', fallback='[#DA70D6]âœ… æª”æ¡ˆä¸Šå‚³å®Œæˆ: {file_path}[/green]', file_path=file_path))
            return True

        except Exception as e:
            console.print(safe_t('error.failed', fallback='[dim #DDA0DD]âŒ ä¸Šå‚³å¤±æ•—: {e}[/red]', e=e))
            return False


# ============================================================================
# 6. å¤šç·šç¨‹è™•ç†æ¡†æ¶
# ============================================================================

class ParallelProcessor:
    """å¤šç·šç¨‹ä¸¦è¡Œè™•ç†å™¨"""

    def __init__(self, max_workers: int = 4):
        """
        åˆå§‹åŒ–ä¸¦è¡Œè™•ç†å™¨

        Args:
            max_workers: æœ€å¤§åŸ·è¡Œç·’æ•¸ï¼Œé è¨­ 4
        """
        self.max_workers = max_workers

    def process_batch(
        self,
        items: List[Any],
        process_func: Callable[[Any], Any],
        description: str = "è™•ç†ä¸­"
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹æ¬¡ä¸¦è¡Œè™•ç†é …ç›®

        Args:
            items: è¦è™•ç†çš„é …ç›®åˆ—è¡¨
            process_func: è™•ç†å‡½æ•¸ï¼Œæ¥æ”¶å–®ä¸€é …ç›®ä¸¦è¿”å›çµæœ
            description: é€²åº¦æè¿°

        Returns:
            çµæœåˆ—è¡¨ï¼Œæ¯å€‹çµæœåŒ…å« {"item", "status", "result"/"error"}

        Example:
            >>> def process_image(path):
            ...     return analyze_image(path)
            >>> processor = ParallelProcessor(max_workers=4)
            >>> results = processor.process_batch(image_paths, process_image, "åˆ†æåœ–ç‰‡")
        """
        results = []

        with Progress(
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("â€¢"),
            TextColumn("{task.completed}/{task.total}"),
            TimeRemainingColumn(),
            console=console
        ) as progress:

            task = progress.add_task(f"âš¡ {description}", total=len(items))

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»å‹™
                futures = {
                    executor.submit(process_func, item): item
                    for item in items
                }

                # æ”¶é›†çµæœ
                for future in as_completed(futures):
                    item = futures[future]

                    try:
                        result = future.result()
                        results.append({
                            "item": item,
                            "status": "success",
                            "result": result
                        })
                    except Exception as e:
                        results.append({
                            "item": item,
                            "status": "error",
                            "error": str(e)
                        })

                    progress.update(task, advance=1)

        # è¼¸å‡ºçµ±è¨ˆ
        success_count = sum(1 for r in results if r["status"] == "success")
        error_count = len(results) - success_count

        console.print(safe_t('error.failed', fallback='\n[#DA70D6]âœ… æˆåŠŸ: {success_count}[/green] | [dim #DDA0DD]âŒ å¤±æ•—: {error_count}[/red]', success_count=success_count, error_count=error_count))

        return results


# ============================================================================
# ä¸»ç¨‹å¼ (æ¸¬è©¦ç”¨)
# ============================================================================

if __name__ == "__main__":
    console.print(Panel(
        """[bold #DDA0DD]Gemini Memory Manager[/bold #DDA0DD]

âœ… è¨˜æ†¶é«”æ± ç®¡ç†å™¨ (MemoryPoolManager)
âœ… å°è©±æ­·å²ç®¡ç†å™¨ (ConversationManager)
âœ… åœ–ç‰‡åˆ†å¡Šè¼‰å…¥ (load_image_chunked)
âœ… å½±ç‰‡åˆ†æ®µè™•ç† (process_video_chunked)
âœ… æª”æ¡ˆä¸Šå‚³æ–·é»çºŒå‚³ (ChunkedUploader)
âœ… å¤šç·šç¨‹è™•ç†æ¡†æ¶ (ParallelProcessor)

[dim]Author: Saki-tw | Email: Saki@saki-studio.com.tw[/dim]""",
        title="ğŸ’¾ Memory Management Tools",
        border_style="#DA70D6"
    ))

    # ç¤ºç¯„è¨˜æ†¶é«”ç®¡ç†å™¨
    mem_manager = MemoryPoolManager(max_memory_mb=2048)
    mem_manager.print_memory_report()
