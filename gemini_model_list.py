#!/usr/bin/env python3
"""
Gemini æ¨¡åž‹åˆ—è¡¨ç®¡ç†å™¨
å¾ž API å‹•æ…‹ç²å–å¯ç”¨æ¨¡åž‹,ä¸¦å¿«å–çµæžœ
"""

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from google import genai


# å¿«å–æª”æ¡ˆè·¯å¾‘
CACHE_DIR = os.path.expanduser('~/.cache/chatgemini')
CACHE_FILE = os.path.join(CACHE_DIR, 'model_list_cache.json')

# å¿«å–æœ‰æ•ˆæœŸï¼ˆ24 å°æ™‚ï¼‰
CACHE_EXPIRY = timedelta(hours=24)


def get_api_models(api_key: str) -> List[Dict]:
    """
    å¾ž Gemini API ç²å–å¯ç”¨æ¨¡åž‹åˆ—è¡¨

    Args:
        api_key: Google API é‡‘é‘°

    Returns:
        æ¨¡åž‹è³‡è¨Šåˆ—è¡¨
    """
    try:
        # ä½¿ç”¨æ–°ç‰ˆçµ±ä¸€ SDK
        client = genai.Client(api_key=api_key)
        models = client.models.list()

        # ç¯©é¸å‡ºæ”¯æ´å°è©±ç”Ÿæˆçš„æ¨¡åž‹
        model_list = []
        for model in models:
            # æ–°ç‰ˆ SDK ä½¿ç”¨ä¸åŒçš„å±¬æ€§åç¨±
            model_name = model.name.replace('models/', '') if hasattr(model, 'name') else str(model)
            supported_methods = getattr(model, 'supported_generation_methods', [])

            if 'generateContent' in supported_methods:
                model_info = {
                    'name': model_name,
                    'display_name': getattr(model, 'display_name', model_name),
                    'description': getattr(model, 'description', ''),
                    'supported_methods': supported_methods,
                }
                model_list.append(model_info)

        return model_list

    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•å¾ž API ç²å–æ¨¡åž‹åˆ—è¡¨: {e}")
        return []


def load_cached_models() -> Optional[Dict]:
    """
    è¼‰å…¥å¿«å–çš„æ¨¡åž‹åˆ—è¡¨

    Returns:
        å¿«å–è³‡æ–™ï¼ˆåŒ…å«æ¨¡åž‹åˆ—è¡¨å’Œæ™‚é–“æˆ³è¨˜ï¼‰æˆ– None
    """
    if not os.path.exists(CACHE_FILE):
        return None

    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)

        # æª¢æŸ¥å¿«å–æ˜¯å¦éŽæœŸ
        cached_time = datetime.fromisoformat(cache['timestamp'])
        if datetime.now() - cached_time > CACHE_EXPIRY:
            return None

        return cache

    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•è¼‰å…¥å¿«å–: {e}")
        return None


def save_models_cache(models: List[Dict]) -> None:
    """
    å„²å­˜æ¨¡åž‹åˆ—è¡¨åˆ°å¿«å–

    Args:
        models: æ¨¡åž‹è³‡è¨Šåˆ—è¡¨
    """
    try:
        os.makedirs(CACHE_DIR, exist_ok=True)

        cache = {
            'timestamp': datetime.now().isoformat(),
            'models': models
        }

        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)

    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•å„²å­˜å¿«å–: {e}")


def get_models(api_key: str, force_refresh: bool = False) -> List[Dict]:
    """
    ç²å–æ¨¡åž‹åˆ—è¡¨ï¼ˆå„ªå…ˆä½¿ç”¨å¿«å–ï¼‰

    Args:
        api_key: Google API é‡‘é‘°
        force_refresh: æ˜¯å¦å¼·åˆ¶é‡æ–°å¾ž API ç²å–

    Returns:
        æ¨¡åž‹è³‡è¨Šåˆ—è¡¨
    """
    # å¦‚æžœä¸å¼·åˆ¶åˆ·æ–°,å…ˆå˜—è©¦è¼‰å…¥å¿«å–
    if not force_refresh:
        cached = load_cached_models()
        if cached:
            return cached['models']

    # å¾ž API ç²å–æœ€æ–°åˆ—è¡¨
    models = get_api_models(api_key)

    # å„²å­˜åˆ°å¿«å–
    if models:
        save_models_cache(models)

    return models


def categorize_models(models: List[Dict]) -> Dict[str, List[Dict]]:
    """
    å°‡æ¨¡åž‹åˆ†é¡ž

    Args:
        models: æ¨¡åž‹è³‡è¨Šåˆ—è¡¨

    Returns:
        åˆ†é¡žå¾Œçš„æ¨¡åž‹å­—å…¸
    """
    categories = {
        'gemini_30': [],      # Gemini 3.0 ç³»åˆ— (NEW!)
        'gemini_25': [],      # Gemini 2.5 ç³»åˆ—
        'gemini_20': [],      # Gemini 2.0 ç³»åˆ—
        'gemini_15': [],      # Gemini 1.5 ç³»åˆ—
        'gemma': [],          # Gemma ç³»åˆ—
        'experimental': [],   # å¯¦é©—æ€§æ¨¡åž‹
        'other': []           # å…¶ä»–æ¨¡åž‹
    }

    for model in models:
        name = model['name'].lower()

        if '3.0' in name or 'gemini-3' in name or '3-0' in name:
            categories['gemini_30'].append(model)
        elif '2.5' in name or 'gemini-2-5' in name:
            categories['gemini_25'].append(model)
        elif '2.0' in name or 'gemini-2-0' in name:
            categories['gemini_20'].append(model)
        elif '1.5' in name or 'gemini-1-5' in name:
            categories['gemini_15'].append(model)
        elif 'gemma' in name:
            categories['gemma'].append(model)
        elif 'exp' in name or 'preview' in name:
            categories['experimental'].append(model)
        else:
            categories['other'].append(model)

    return categories


def get_recommended_models(models: List[Dict]) -> List[Dict]:
    """
    ç²å–æŽ¨è–¦çš„ä¸»è¦æ¨¡åž‹

    Args:
        models: å®Œæ•´æ¨¡åž‹åˆ—è¡¨

    Returns:
        æŽ¨è–¦æ¨¡åž‹åˆ—è¡¨ï¼ˆ3-5 å€‹ï¼‰
    """
    # æŽ¨è–¦æ¨¡åž‹çš„å„ªå…ˆé †åºï¼ˆ2025-11-29 æ›´æ–°ï¼‰
    priority_names = [
        'gemini-3-pro-preview',      # æœ€æ–°æœ€å¼· (2025-11-18)
        'gemini-2.5-flash',          # æ€§åƒ¹æ¯”æœ€é«˜
        'gemini-2.5-pro',            # æœ€å¼·æŽ¨ç†
        'gemini-2.5-flash-lite',     # è¼•é‡å¿«é€Ÿ
        'gemini-2.0-flash',          # ç©©å®šç‰ˆ
    ]

    recommended = []

    # æŒ‰å„ªå…ˆé †åºæŸ¥æ‰¾
    for name in priority_names:
        for model in models:
            if model['name'] == name:
                recommended.append(model)
                break

    return recommended


def format_model_display(model: Dict, index: int = None) -> str:
    """
    æ ¼å¼åŒ–æ¨¡åž‹é¡¯ç¤ºæ–‡å­—

    Args:
        model: æ¨¡åž‹è³‡è¨Š
        index: é¸é …ç·¨è™Ÿï¼ˆå¯é¸ï¼‰

    Returns:
        æ ¼å¼åŒ–çš„é¡¯ç¤ºæ–‡å­—
    """
    name = model['name']

    # ç”Ÿæˆå‹å–„çš„é¡¯ç¤ºåç¨±
    if 'gemini-3' in name and 'pro' in name:
        display = "âœ¨ 3.0 Pro Previewï¼ˆæœ€æ–°æœ€å¼·ï¼‰"
    elif 'flash-lite' in name:
        display = "Flash Liteï¼ˆè¼•é‡ç‰ˆï¼‰"
    elif 'flash' in name and '2.5' in name:
        display = "Flashï¼ˆå¿«é€Ÿç‰ˆï¼‰"
    elif 'pro' in name and '2.5' in name:
        display = "Proï¼ˆå¼·å¤§ç‰ˆï¼‰"
    elif 'flash' in name and '2.0' in name:
        display = "2.0 Flash"
    else:
        display = model.get('display_name', name)

    if index is not None:
        return f"[{index}] {display}"
    else:
        return display


def initialize_models(api_key: str) -> bool:
    """
    åˆå§‹åŒ–æ¨¡åž‹åˆ—è¡¨ï¼ˆåˆæ¬¡é‹è¡Œæ™‚èª¿ç”¨ï¼‰

    Args:
        api_key: Google API é‡‘é‘°

    Returns:
        æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
    """
    print("ðŸ”„ æ­£åœ¨å¾ž API ç²å–æœ€æ–°æ¨¡åž‹åˆ—è¡¨...")

    models = get_models(api_key, force_refresh=True)

    if models:
        print(f"âœ… æˆåŠŸç²å– {len(models)} å€‹å¯ç”¨æ¨¡åž‹")

        # é¡¯ç¤ºæŽ¨è–¦æ¨¡åž‹
        recommended = get_recommended_models(models)
        print(f"\næŽ¨è–¦æ¨¡åž‹ ({len(recommended)} å€‹):")
        for i, model in enumerate(recommended, 1):
            print(f"  {i}. {model['name']}")

        return True
    else:
        print("âŒ ç„¡æ³•ç²å–æ¨¡åž‹åˆ—è¡¨")
        return False


# é è¨­çš„å¾Œå‚™æ¨¡åž‹åˆ—è¡¨ï¼ˆç•¶ API ç„¡æ³•è¨ªå•æ™‚ä½¿ç”¨ï¼‰
FALLBACK_MODELS = [
    {
        'name': 'gemini-3-pro-preview',
        'display_name': 'Gemini 3.0 Pro Preview',
        'description': 'æœ€æ–°æœ€å¼·çš„æ¨¡åž‹',
    },
    {
        'name': 'gemini-2.5-flash',
        'display_name': 'Gemini 2.5 Flash',
        'description': 'å¿«é€Ÿä¸”æ™ºæ…§çš„æ¨¡åž‹',
    },
    {
        'name': 'gemini-2.5-pro',
        'display_name': 'Gemini 2.5 Pro',
        'description': 'æœ€å¼·å¤§çš„æ¨¡åž‹',
    },
    {
        'name': 'gemini-2.5-flash-lite',
        'display_name': 'Gemini 2.5 Flash Lite',
        'description': 'è¼•é‡ç‰ˆæ¨¡åž‹',
    },
]


class GeminiModelList:
    """
    Gemini æ¨¡åž‹åˆ—è¡¨ç®¡ç†å™¨ï¼ˆé¡žåˆ¥å°è£ç‰ˆï¼‰
    æä¾›å‹•æ…‹æ¨¡åž‹ç²å–ã€å¿«å–ç®¡ç†å’Œåˆ†é¡žåŠŸèƒ½
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¨¡åž‹ç®¡ç†å™¨

        Args:
            api_key: Google API é‡‘é‘°ï¼ˆå¦‚æžœæœªæä¾›,å¾žç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
        """
        self.api_key = api_key or os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY')
        self._models_cache = None

    def get_models(self, force_refresh: bool = False) -> List[Dict]:
        """
        ç²å–æ¨¡åž‹åˆ—è¡¨ï¼ˆå„ªå…ˆä½¿ç”¨å¿«å–ï¼‰

        Args:
            force_refresh: æ˜¯å¦å¼·åˆ¶é‡æ–°å¾ž API ç²å–

        Returns:
            æ¨¡åž‹è³‡è¨Šåˆ—è¡¨
        """
        if self._models_cache and not force_refresh:
            return self._models_cache

        # å¦‚æžœä¸å¼·åˆ¶åˆ·æ–°,å…ˆå˜—è©¦è¼‰å…¥å¿«å–
        if not force_refresh:
            cached = load_cached_models()
            if cached:
                self._models_cache = cached['models']
                return self._models_cache

        # å¾ž API ç²å–æœ€æ–°åˆ—è¡¨
        models = get_api_models(self.api_key)

        # å„²å­˜åˆ°å¿«å–
        if models:
            save_models_cache(models)
            self._models_cache = models
        else:
            # é™ç´šåˆ°å¾Œå‚™åˆ—è¡¨
            self._models_cache = FALLBACK_MODELS

        return self._models_cache

    def update_models(self, force: bool = False) -> bool:
        """
        æ›´æ–°æ¨¡åž‹åˆ—è¡¨

        Args:
            force: æ˜¯å¦å¼·åˆ¶æ›´æ–°

        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        try:
            models = self.get_models(force_refresh=force)
            return len(models) > 0
        except Exception as e:
            print(f"âš ï¸  æ›´æ–°æ¨¡åž‹åˆ—è¡¨å¤±æ•—: {e}")
            return False

    def get_cache_info(self) -> Dict:
        """
        ç²å–å¿«å–è³‡è¨Š

        Returns:
            å¿«å–è³‡è¨Šå­—å…¸ï¼ˆåŒ…å« existsã€countã€last_updateã€timestamp ç­‰ï¼‰
        """
        cached = load_cached_models()
        if cached:
            timestamp_str = cached['timestamp']
            try:
                dt = datetime.fromisoformat(timestamp_str)
                last_update = dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                last_update = timestamp_str

            return {
                'exists': True,
                'count': len(cached['models']),
                'timestamp': timestamp_str,
                'last_update': last_update,
                'is_expired': False
            }

        # æª¢æŸ¥æ˜¯å¦æœ‰éŽæœŸå¿«å–
        if os.path.exists(CACHE_FILE):
            try:
                with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                timestamp_str = cache.get('timestamp', 'unknown')
                try:
                    dt = datetime.fromisoformat(timestamp_str)
                    last_update = dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    last_update = timestamp_str

                return {
                    'exists': True,
                    'count': len(cache.get('models', [])),
                    'timestamp': timestamp_str,
                    'last_update': last_update,
                    'is_expired': True
                }
            except:
                pass

        return {
            'exists': False,
            'count': 0,
            'timestamp': None,
            'last_update': None,
            'is_expired': False
        }

    def categorize_models(self, models: Optional[List[Dict]] = None) -> Dict[str, List[Dict]]:
        """
        å°‡æ¨¡åž‹åˆ†é¡ž

        Args:
            models: æ¨¡åž‹è³‡è¨Šåˆ—è¡¨ï¼ˆå¦‚æžœæœªæä¾›,ä½¿ç”¨å¿«å–ï¼‰

        Returns:
            åˆ†é¡žå¾Œçš„æ¨¡åž‹å­—å…¸
        """
        if models is None:
            models = self.get_models()

        return categorize_models(models)

    def get_recommended_models(self, models: Optional[List[Dict]] = None) -> List[Dict]:
        """
        ç²å–æŽ¨è–¦çš„ä¸»è¦æ¨¡åž‹

        Args:
            models: å®Œæ•´æ¨¡åž‹åˆ—è¡¨ï¼ˆå¦‚æžœæœªæä¾›,ä½¿ç”¨å¿«å–ï¼‰

        Returns:
            æŽ¨è–¦æ¨¡åž‹åˆ—è¡¨ï¼ˆ3-5 å€‹ï¼‰
        """
        if models is None:
            models = self.get_models()

        return get_recommended_models(models)

    def get_all_models(self) -> List[str]:
        """
        ç²å–æ‰€æœ‰æ¨¡åž‹åç¨±åˆ—è¡¨ï¼ˆç”¨æ–¼æ¨¡åž‹é¸æ“‡å™¨ï¼‰

        Returns:
            æ¨¡åž‹åç¨±åˆ—è¡¨
        """
        models = self.get_models()
        return [model['name'] for model in models]

    def format_model_display(self, model: Dict, index: int = None) -> str:
        """
        æ ¼å¼åŒ–æ¨¡åž‹é¡¯ç¤ºæ–‡å­—

        Args:
            model: æ¨¡åž‹è³‡è¨Š
            index: é¸é …ç·¨è™Ÿï¼ˆå¯é¸ï¼‰

        Returns:
            æ ¼å¼åŒ–çš„é¡¯ç¤ºæ–‡å­—
        """
        return format_model_display(model, index)


if __name__ == '__main__':
    """æ¸¬è©¦æ¨¡çµ„"""
    import sys

    api_key = os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY')

    if not api_key:
        print("âŒ è«‹è¨­å®š GOOGLE_API_KEY æˆ– GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸")
        sys.exit(1)

    print("=" * 70)
    print("ðŸ§ª Gemini æ¨¡åž‹åˆ—è¡¨æ¸¬è©¦")
    print("=" * 70)

    # æ¸¬è©¦é¡žåˆ¥å°è£
    print("\næ¸¬è©¦ GeminiModelList é¡žåˆ¥:")
    print("-" * 70)
    manager = GeminiModelList(api_key)

    # æ¸¬è©¦ç²å–æ¨¡åž‹
    models = manager.get_models()
    print(f"\nç¸½å…± {len(models)} å€‹æ¨¡åž‹\n")

    # æ¸¬è©¦å¿«å–è³‡è¨Š
    cache_info = manager.get_cache_info()
    print(f"å¿«å–è³‡è¨Š: {cache_info}\n")

    # æ¸¬è©¦åˆ†é¡ž
    categories = manager.categorize_models()

    for category, model_list in categories.items():
        if model_list:
            print(f"\n{category.upper()} ({len(model_list)} å€‹):")
            for model in model_list[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
                print(f"  - {model['name']}")
            if len(model_list) > 5:
                print(f"  ... é‚„æœ‰ {len(model_list) - 5} å€‹")

    # æ¸¬è©¦æŽ¨è–¦æ¨¡åž‹
    print("\n" + "=" * 70)
    print("æŽ¨è–¦æ¨¡åž‹:")
    print("=" * 70)
    recommended = manager.get_recommended_models()
    for i, model in enumerate(recommended, 1):
        print(f"{i}. {model['name']}")
