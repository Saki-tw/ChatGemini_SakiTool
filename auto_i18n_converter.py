#!/usr/bin/env python3
"""
è‡ªå‹• i18n è½‰æ›å·¥å…·

åŠŸèƒ½ï¼š
1. æƒæ Python æª”æ¡ˆä¸­çš„ç¡¬ç·¨ç¢¼ä¸­æ–‡
2. è‡ªå‹•ç”Ÿæˆèªç¾©åŒ–ç¿»è­¯éµ
3. æ‰¹æ¬¡æ›¿æ›ç‚º safe_t() èª¿ç”¨
4. ç”Ÿæˆèªè¨€åŒ…æ¢ç›®æ¨¡æ¿
5. ï¼ˆå¯é¸ï¼‰ä½¿ç”¨ AI è‡ªå‹•ç¿»è­¯å…¶ä»–èªè¨€

ä½œè€…: Saki-tw (with Claude Code)
æ—¥æœŸ: 2025-10-29
"""

import re
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json

class AutoI18nConverter:
    def __init__(self, dry_run: bool = True):
        """
        åˆå§‹åŒ–è½‰æ›å™¨

        Args:
            dry_run: æ˜¯å¦ç‚ºæ¨¡æ“¬åŸ·è¡Œï¼ˆä¸å¯¦éš›ä¿®æ”¹æª”æ¡ˆï¼‰
        """
        self.dry_run = dry_run
        self.translations = {}  # {key: {lang: translation}}
        self.conversions = []   # è½‰æ›è¨˜éŒ„

        # ç¿»è­¯éµè¨ˆæ•¸å™¨ï¼ˆç”¨æ–¼ç”Ÿæˆå”¯ä¸€ keyï¼‰
        self.key_counters = {}

    def generate_key(self, context: str, text: str, line_num: int) -> str:
        """
        ç”Ÿæˆèªç¾©åŒ–çš„ç¿»è­¯éµ

        ç­–ç•¥ï¼š
        1. å¾æ–‡ä»¶è·¯å¾‘æå–æ¨¡çµ„åç¨±
        2. å¾ä»£ç¢¼ä¸Šä¸‹æ–‡æ¨æ–·é¡åˆ¥
        3. å¾æ–‡å­—å…§å®¹æå–é—œéµè©
        4. ç¢ºä¿å”¯ä¸€æ€§

        Args:
            context: ä¸Šä¸‹æ–‡ï¼ˆæª”æ¡ˆè·¯å¾‘ã€å‡½æ•¸åç¨±ç­‰ï¼‰
            text: ä¸­æ–‡æ–‡å­—
            line_num: è¡Œè™Ÿ

        Returns:
            ç¿»è­¯éµï¼Œä¾‹å¦‚ï¼š'chat.system.config_loaded'
        """
        # æå–æ¨¡çµ„åç¨±
        if 'gemini_chat.py' in context:
            module = 'chat'
        elif 'CodeGemini.py' in context:
            module = 'codegemini'
        elif 'CodeGemini/' in context:
            submodule = context.split('CodeGemini/')[1].split('.py')[0]
            module = f'codegemini.{submodule.replace("/", ".")}'
        else:
            filename = Path(context).stem
            module = filename.replace('gemini_', '').replace('_', '')

        # åˆ†é¡
        category = self._classify_message(text)

        # æå–é—œéµè©
        keyword = self._extract_keyword(text)

        # çµ„åˆéµ
        base_key = f"{module}.{category}.{keyword}"

        # ç¢ºä¿å”¯ä¸€æ€§
        if base_key in self.key_counters:
            self.key_counters[base_key] += 1
            return f"{base_key}_{self.key_counters[base_key]}"
        else:
            self.key_counters[base_key] = 0
            return base_key

    def _classify_message(self, text: str) -> str:
        """åˆ†é¡è¨Šæ¯é¡å‹"""
        text_lower = text.lower()

        # ç³»çµ±è¨Šæ¯
        if any(kw in text for kw in ['âœ…', 'å·²è¼‰å…¥', 'å•Ÿç”¨', 'åˆå§‹åŒ–', 'é…ç½®']):
            return 'system'

        # éŒ¯èª¤è¨Šæ¯
        if any(kw in text for kw in ['âŒ', 'éŒ¯èª¤', 'å¤±æ•—', 'ç„¡æ³•', 'ä¸å­˜åœ¨']):
            return 'error'

        # è­¦å‘Šè¨Šæ¯
        if any(kw in text for kw in ['âš ï¸', 'è­¦å‘Š', 'æ³¨æ„', 'å»ºè­°']):
            return 'warning'

        # å¹«åŠ©è¨Šæ¯
        if any(kw in text for kw in ['æŒ‡ä»¤', 'å‘½ä»¤', 'ç”¨æ³•', 'èªªæ˜', 'ç¯„ä¾‹']):
            return 'help'

        # æç¤ºè¨Šæ¯
        if any(kw in text for kw in ['è«‹', 'è¼¸å…¥', 'é¸æ“‡', 'ç¢ºèª']):
            return 'prompt'

        # ç‹€æ…‹è¨Šæ¯
        if any(kw in text for kw in ['è™•ç†ä¸­', 'è¼‰å…¥ä¸­', 'å®Œæˆ', 'æˆåŠŸ']):
            return 'status'

        # é è¨­ç‚ºä¸€èˆ¬è¨Šæ¯
        return 'message'

    def _extract_keyword(self, text: str) -> str:
        """å¾æ–‡å­—ä¸­æå–é—œéµè©"""
        # ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿå’Œç‰¹æ®Šç¬¦è™Ÿ
        clean_text = re.sub(r'[âœ…âŒâš ï¸ğŸ’¾ğŸ§ ğŸ”§ğŸ“ğŸ“ŠğŸ¨ğŸ”]', '', text)
        clean_text = re.sub(r'[\s\(\)ï¼ˆï¼‰ï¼š:ï¼šï¼Œ,ã€‚.ï¼!ï¼Ÿ?]', '_', clean_text)

        # ç§»é™¤é€£çºŒçš„åº•ç·š
        clean_text = re.sub(r'_+', '_', clean_text)
        clean_text = clean_text.strip('_')

        # é™åˆ¶é•·åº¦
        if len(clean_text) > 30:
            # å–å‰å¹¾å€‹é—œéµè©
            words = clean_text.split('_')
            clean_text = '_'.join(words[:3])

        # è½‰ç‚ºå°å¯«ï¼ˆä¿ç•™ä¸­æ–‡ï¼‰
        # åªè½‰æ›è‹±æ–‡éƒ¨åˆ†
        result = ''
        for char in clean_text:
            if 'A' <= char <= 'Z':
                result += char.lower()
            else:
                result += char

        return result or 'msg'

    def find_chinese_strings(self, file_path: Path) -> List[Tuple[int, str, str]]:
        """
        æ‰¾å‡ºæª”æ¡ˆä¸­çš„ç¡¬ç·¨ç¢¼ä¸­æ–‡å­—ä¸²

        Returns:
            List of (line_number, original_line, chinese_text)
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')

            results = []
            for i, line in enumerate(lines, 1):
                # è·³éè¨»è§£
                if line.strip().startswith('#'):
                    continue

                # è·³éå·²ä½¿ç”¨ i18n çš„è¡Œ
                if 'safe_t(' in line or re.search(r'\bt\(', line):
                    continue

                # è·³é docstring
                if '"""' in line or "'''" in line:
                    continue

                # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ä¸”ç‚ºç”¨æˆ¶å¯è¦‹
                if re.search(r'[\u4e00-\u9fff]', line):
                    if self._is_user_visible(line):
                        # æå–ä¸­æ–‡å­—ä¸²
                        chinese_texts = self._extract_chinese_texts(line)
                        for text in chinese_texts:
                            results.append((i, line, text))

            return results

        except Exception as e:
            print(f"âš ï¸  è®€å–å¤±æ•—: {file_path} - {e}")
            return []

    def _is_user_visible(self, line: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºç”¨æˆ¶å¯è¦‹è¨Šæ¯"""
        line_lower = line.lower()
        return any(pattern in line_lower for pattern in [
            'print(', 'console.print(', 'logger.info(',
            'logger.warning(', 'logger.error(', 'rich.print(',
            'click.echo(', 'sys.stdout.write(', 'sys.stderr.write('
        ])

    def _extract_chinese_texts(self, line: str) -> List[str]:
        """å¾è¡Œä¸­æå–æ‰€æœ‰ä¸­æ–‡å­—ä¸²"""
        # åŒ¹é…å­—ä¸²å­—é¢é‡
        patterns = [
            r'"([^"]*[\u4e00-\u9fff][^"]*)"',  # é›™å¼•è™Ÿ
            r"'([^']*[\u4e00-\u9fff][^']*)'",  # å–®å¼•è™Ÿ
        ]

        texts = []
        for pattern in patterns:
            matches = re.findall(pattern, line)
            texts.extend(matches)

        return texts

    def convert_line(self, original_line: str, chinese_text: str,
                    translation_key: str) -> str:
        """
        è½‰æ›å–®è¡Œä»£ç¢¼

        Args:
            original_line: åŸå§‹è¡Œ
            chinese_text: è¦æ›¿æ›çš„ä¸­æ–‡æ–‡å­—
            translation_key: ç¿»è­¯éµ

        Returns:
            è½‰æ›å¾Œçš„è¡Œ
        """
        # æª¢æŸ¥æ˜¯å¦æœ‰æ ¼å¼åŒ–åƒæ•¸
        has_format = '{' in chinese_text and '}' in chinese_text

        # æå–æ ¼å¼åŒ–åƒæ•¸
        format_params = []
        if has_format:
            format_params = re.findall(r'\{(\w+)\}', chinese_text)

        # æ§‹å»º safe_t() èª¿ç”¨
        if format_params:
            params_str = ', '.join(f'{p}={p}' for p in format_params)
            safe_t_call = f"safe_t('{translation_key}', fallback='{chinese_text}', {params_str})"
        else:
            safe_t_call = f"safe_t('{translation_key}', fallback='{chinese_text}')"

        # æ›¿æ›åŸå§‹å­—ä¸²
        # è™•ç†é›™å¼•è™Ÿå’Œå–®å¼•è™Ÿ
        for quote in ['"', "'"]:
            pattern = f'{quote}{re.escape(chinese_text)}{quote}'
            if pattern.replace('\\', '') in original_line:
                converted_line = original_line.replace(
                    f'{quote}{chinese_text}{quote}',
                    safe_t_call
                )
                return converted_line

        # å¦‚æœç›´æ¥æ›¿æ›å¤±æ•—ï¼Œè¿”å›åŸå§‹è¡Œ
        return original_line

    def convert_file(self, file_path: Path) -> Dict:
        """
        è½‰æ›æ•´å€‹æª”æ¡ˆ

        Returns:
            è½‰æ›çµ±è¨ˆè³‡æ–™
        """
        print(f"\nğŸ“„ è™•ç†: {file_path}")

        # æ‰¾å‡ºæ‰€æœ‰ç¡¬ç·¨ç¢¼ä¸­æ–‡
        chinese_strings = self.find_chinese_strings(file_path)

        if not chinese_strings:
            print(f"   âœ… ç„¡éœ€è½‰æ›")
            return {'converted': 0, 'skipped': 0}

        print(f"   ç™¼ç¾ {len(chinese_strings)} è™•ç¡¬ç·¨ç¢¼")

        # è®€å–æª”æ¡ˆå…§å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        converted_count = 0
        skipped_count = 0

        # è½‰æ›æ¯ä¸€è¡Œ
        for line_num, original_line, chinese_text in chinese_strings:
            # ç”Ÿæˆç¿»è­¯éµ
            context = str(file_path)
            translation_key = self.generate_key(context, chinese_text, line_num)

            # è½‰æ›è¡Œ
            converted_line = self.convert_line(
                original_line,
                chinese_text,
                translation_key
            )

            if converted_line != original_line:
                # è¨˜éŒ„è½‰æ›
                lines[line_num - 1] = converted_line

                # è¨˜éŒ„ç¿»è­¯
                if translation_key not in self.translations:
                    self.translations[translation_key] = {
                        'zh-TW': chinese_text
                    }

                self.conversions.append({
                    'file': str(file_path),
                    'line': line_num,
                    'key': translation_key,
                    'original': original_line.strip(),
                    'converted': converted_line.strip()
                })

                converted_count += 1
                print(f"   Line {line_num:4d}: {translation_key}")
            else:
                skipped_count += 1

        # å¯«å›æª”æ¡ˆï¼ˆå¦‚æœä¸æ˜¯ dry-runï¼‰
        if not self.dry_run and converted_count > 0:
            # ç¢ºä¿æª”æ¡ˆé–‹é ­æœ‰ safe_t å°å…¥
            self._ensure_safe_t_import(lines)

            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)

            print(f"   âœ… å·²è½‰æ› {converted_count} è™•")
        else:
            print(f"   ğŸ” [DRY-RUN] å°‡è½‰æ› {converted_count} è™•")

        return {'converted': converted_count, 'skipped': skipped_count}

    def _ensure_safe_t_import(self, lines: List[str]) -> None:
        """ç¢ºä¿æª”æ¡ˆé–‹é ­æœ‰ safe_t å°å…¥"""
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰å°å…¥
        has_import = any('from utils import' in line and 'safe_t' in line
                        for line in lines[:30])  # åªæª¢æŸ¥å‰ 30 è¡Œ

        if not has_import:
            # å°‹æ‰¾åˆé©çš„æ’å…¥ä½ç½®ï¼ˆåœ¨å…¶ä»– import ä¹‹å¾Œï¼‰
            insert_pos = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('import ') or line.strip().startswith('from '):
                    insert_pos = i + 1

            # æ’å…¥å°å…¥
            lines.insert(insert_pos, 'from utils import safe_t\n')

    def export_translations(self, output_file: str = 'i18n_translations_new.yaml'):
        """åŒ¯å‡ºç¿»è­¯éµ"""
        import yaml

        print(f"\nğŸ“ åŒ¯å‡ºç¿»è­¯éµè‡³: {output_file}")
        print(f"   ç¸½è¨ˆ: {len(self.translations)} å€‹éµ")

        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(self.translations, f, allow_unicode=True, sort_keys=False)

    def export_report(self, output_file: str = 'i18n_conversion_report.json'):
        """åŒ¯å‡ºè½‰æ›å ±å‘Š"""
        print(f"\nğŸ“Š åŒ¯å‡ºè½‰æ›å ±å‘Šè‡³: {output_file}")

        report = {
            'total_conversions': len(self.conversions),
            'total_keys': len(self.translations),
            'conversions': self.conversions
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»ç¨‹å¼"""
    import argparse

    parser = argparse.ArgumentParser(
        description='è‡ªå‹•è½‰æ›ç¡¬ç·¨ç¢¼ä¸­æ–‡è‡³ i18n'
    )
    parser.add_argument(
        'files',
        nargs='+',
        help='è¦è½‰æ›çš„æª”æ¡ˆè·¯å¾‘'
    )
    parser.add_argument(
        '--execute',
        action='store_true',
        help='å¯¦éš›åŸ·è¡Œè½‰æ›ï¼ˆé è¨­ç‚º dry-runï¼‰'
    )

    args = parser.parse_args()

    # å‰µå»ºè½‰æ›å™¨
    converter = AutoI18nConverter(dry_run=not args.execute)

    if converter.dry_run:
        print("\nâš ï¸  DRY-RUN æ¨¡å¼ï¼šä¸æœƒå¯¦éš›ä¿®æ”¹æª”æ¡ˆ")
    else:
        print("\nğŸ”§ åŸ·è¡Œæ¨¡å¼ï¼šå°‡å¯¦éš›ä¿®æ”¹æª”æ¡ˆ")

    print("=" * 80)

    # è½‰æ›æ¯å€‹æª”æ¡ˆ
    total_converted = 0
    total_skipped = 0

    for file_path_str in args.files:
        file_path = Path(file_path_str)
        if file_path.exists():
            stats = converter.convert_file(file_path)
            total_converted += stats['converted']
            total_skipped += stats['skipped']
        else:
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")

    # åŒ¯å‡ºçµæœ
    print("\n" + "=" * 80)
    print("ğŸ“Š è½‰æ›ç¸½çµ")
    print("=" * 80)
    print(f"ç¸½è¨ˆè½‰æ›: {total_converted} è™•")
    print(f"ç¸½è¨ˆè·³é: {total_skipped} è™•")
    print(f"æ–°å¢ç¿»è­¯éµ: {len(converter.translations)} å€‹")

    if converter.conversions:
        converter.export_translations()
        converter.export_report()

    print("\nâœ… å®Œæˆ")


if __name__ == "__main__":
    main()
