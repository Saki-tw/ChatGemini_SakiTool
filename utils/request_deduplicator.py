#!/usr/bin/env python3
"""
è«‹æ±‚å»é‡å™¨
ä½¿ç”¨ LRU å¿«å–é¿å…é‡è¤‡çš„ API èª¿ç”¨

ç‰¹æ€§ï¼š
1. SHA256 hash ç”Ÿæˆå»é‡éµ
2. LRU å¿«å–ï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰
3. TTL éæœŸæ©Ÿåˆ¶ï¼ˆé˜²æ­¢éæœŸè³‡æ–™ï¼‰
4. ä¸¦ç™¼è«‹æ±‚å…±äº«ï¼ˆç›¸åŒè«‹æ±‚åªèª¿ç”¨ä¸€æ¬¡ï¼‰
5. çµ±è¨ˆè³‡è¨Šæ”¶é›†ï¼ˆå‘½ä¸­ç‡ã€å¿«å–å¤§å°ç­‰ï¼‰

ä½œè€…ï¼šClaude Code (Sonnet 4.5)
æ—¥æœŸï¼š2025-10-25
ç‰ˆæœ¬ï¼š1.0.0
"""
import asyncio
import hashlib
import re
import time
from collections import OrderedDict
from typing import Callable, Any, Optional, Dict
from rich.console import Console

console = Console()


# ==================== LRU å¿«å– ====================

class LRUCache:
    """
    LRUï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰å¿«å–

    ç‰¹æ€§ï¼š
    - å›ºå®šå®¹é‡ï¼ˆé”åˆ°ä¸Šé™æ™‚ç§»é™¤æœ€èˆŠé …ç›®ï¼‰
    - TTL éæœŸï¼ˆè¶…éæ™‚é–“çš„é …ç›®è‡ªå‹•ç§»é™¤ï¼‰
    - O(1) æŸ¥è©¢å’Œæ›´æ–°
    """

    def __init__(self, max_size: int = 1000, ttl: int = 300):
        """
        åˆå§‹åŒ– LRU å¿«å–

        Args:
            max_size: æœ€å¤§å¿«å–é …ç›®æ•¸
            ttl: å¿«å–æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        """
        self.cache: OrderedDict[str, Any] = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl
        self.timestamps: Dict[str, float] = {}

    def get(self, key: str) -> Optional[Any]:
        """
        ç²å–å¿«å–é …ç›®

        Args:
            key: å¿«å–éµ

        Returns:
            å¿«å–å€¼ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœªéæœŸï¼‰ï¼Œå¦å‰‡ None
        """
        # æª¢æŸ¥ TTL
        if key in self.timestamps:
            if time.time() - self.timestamps[key] > self.ttl:
                self.remove(key)
                return None

        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨
        if key not in self.cache:
            return None

        # å‘½ä¸­ï¼Œç§»åˆ°æœ€æ–°ï¼ˆOrderedDict æœƒç¶­è­·é †åºï¼‰
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: str, value: Any):
        """
        æ–°å¢/æ›´æ–°å¿«å–é …ç›®

        Args:
            key: å¿«å–éµ
            value: å¿«å–å€¼
        """
        # å·²å­˜åœ¨ï¼Œæ›´æ–°
        if key in self.cache:
            self.cache.move_to_end(key)
            self.cache[key] = value
            self.timestamps[key] = time.time()
            return

        # æª¢æŸ¥å®¹é‡
        if len(self.cache) >= self.max_size:
            # ç§»é™¤æœ€èˆŠçš„ï¼ˆOrderedDict çš„ç¬¬ä¸€å€‹ï¼‰
            oldest_key = next(iter(self.cache))
            self.remove(oldest_key)

        # æ–°å¢
        self.cache[key] = value
        self.timestamps[key] = time.time()
        self.cache.move_to_end(key)

    def remove(self, key: str):
        """
        ç§»é™¤å¿«å–é …ç›®

        Args:
            key: å¿«å–éµ
        """
        if key in self.cache:
            del self.cache[key]
        if key in self.timestamps:
            del self.timestamps[key]

    def clear(self):
        """æ¸…ç©ºå¿«å–"""
        self.cache.clear()
        self.timestamps.clear()

    def size(self) -> int:
        """ç²å–ç•¶å‰å¿«å–å¤§å°"""
        return len(self.cache)


# ==================== å»é‡å™¨ ====================

class RequestDeduplicator:
    """
    è«‹æ±‚å»é‡å™¨

    è‡ªå‹•æª¢æ¸¬ä¸¦é¿å…é‡è¤‡çš„ API èª¿ç”¨ã€‚
    ä½¿ç”¨ LRU å¿«å–å­˜å„²æœ€è¿‘çš„çµæœï¼Œä¸¦åœ¨æ”¶åˆ°ç›¸åŒè«‹æ±‚æ™‚ç›´æ¥è¿”å›å¿«å–ã€‚

    ä½¿ç”¨ç¯„ä¾‹ï¼š
        deduper = RequestDeduplicator(
            max_cache_size=1000,
            ttl=300
        )

        # å®šç¾© fetch å‡½æ•¸
        async def fetch_result():
            response = await api.generate_content(...)
            return response.text

        # ç²å–çµæœï¼ˆè‡ªå‹•å»é‡ï¼‰
        result = await deduper.get_or_fetch(
            model='gemini-2.5-flash',
            content='åˆ†æé€™å¼µåœ–ç‰‡',
            fetch_func=fetch_result
        )
    """

    def __init__(
        self,
        max_cache_size: int = 1000,
        ttl: int = 300,
        enabled: bool = True,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–å»é‡å™¨

        Args:
            max_cache_size: æœ€å¤§å¿«å–é …ç›®æ•¸
            ttl: å¿«å–æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
            enabled: æ˜¯å¦å•Ÿç”¨å»é‡
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š
        """
        self.cache = LRUCache(max_cache_size, ttl)
        self.enabled = enabled
        self.verbose = verbose

        # æ­£åœ¨åŸ·è¡Œçš„è«‹æ±‚ {key: Future}
        # ç”¨æ–¼ä¸¦ç™¼è«‹æ±‚å…±äº«ï¼ˆç›¸åŒè«‹æ±‚åŒæ™‚åˆ°é”æ™‚ï¼‰
        self.pending: Dict[str, asyncio.Task] = {}

        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'dedup_hits': 0,  # ä¸¦ç™¼å»é‡
            'cache_misses': 0
        }

        # é–ï¼ˆä¿è­· pendingï¼‰
        self._lock = asyncio.Lock()

    async def get_or_fetch(
        self,
        model: str,
        content: str,
        fetch_func: Callable[[], Any],
        config: Optional[Dict] = None
    ) -> Any:
        """
        ç²å–çµæœï¼ˆå„ªå…ˆä½¿ç”¨å¿«å–ï¼‰

        æµç¨‹ï¼š
        1. æª¢æŸ¥å¿«å–
        2. æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒè«‹æ±‚æ­£åœ¨åŸ·è¡Œ
        3. åŸ·è¡Œæ–°è«‹æ±‚ä¸¦å¿«å–çµæœ

        Args:
            model: æ¨¡å‹åç¨±
            content: è«‹æ±‚å…§å®¹
            fetch_func: ç²å–çµæœçš„å‡½æ•¸ï¼ˆç„¡åƒæ•¸ï¼‰
            config: é…ç½®åƒæ•¸ï¼ˆå¯é¸ï¼Œç”¨æ–¼ç”Ÿæˆå»é‡éµï¼‰

        Returns:
            API å›æ‡‰çµæœ
        """
        if not self.enabled:
            # å»é‡å™¨åœç”¨ï¼Œç›´æ¥èª¿ç”¨
            return await fetch_func()

        self.stats['total_requests'] += 1

        # ç”Ÿæˆå»é‡éµ
        key = self._generate_key(model, content, config)

        # æª¢æŸ¥å¿«å–
        cached = self.cache.get(key)
        if cached is not None:
            self.stats['cache_hits'] += 1
            if self.verbose:
                console.print(f"[dim]ğŸ’¾ å¿«å–å‘½ä¸­ï¼ˆå»é‡ï¼‰[/dim]")
            return cached

        # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒè«‹æ±‚æ­£åœ¨åŸ·è¡Œ
        async with self._lock:
            if key in self.pending:
                self.stats['dedup_hits'] += 1
                if self.verbose:
                    console.print(f"[dim]ğŸ”— ç­‰å¾…ç¾æœ‰è«‹æ±‚å®Œæˆï¼ˆä¸¦ç™¼å»é‡ï¼‰[/dim]")

                # ç­‰å¾…ç¾æœ‰è«‹æ±‚å®Œæˆ
                task = self.pending[key]

        # å¦‚æœæœ‰æ­£åœ¨åŸ·è¡Œçš„è«‹æ±‚ï¼Œç­‰å¾…å®ƒ
        if key in self.pending:
            try:
                result = await task
                return result
            except Exception as e:
                # å¦‚æœç¾æœ‰è«‹æ±‚å¤±æ•—ï¼Œä¸ä½¿ç”¨å…¶çµæœ
                pass

        # å¿«å–æœªå‘½ä¸­ï¼ŒåŸ·è¡Œæ–°è«‹æ±‚
        self.stats['cache_misses'] += 1

        async with self._lock:
            # å†æ¬¡æª¢æŸ¥ï¼ˆé¿å…ç«¶çˆ­æ¢ä»¶ï¼‰
            if key in self.pending:
                task = self.pending[key]
                return await task

            # å‰µå»ºæ–°ä»»å‹™
            task = asyncio.create_task(fetch_func())
            self.pending[key] = task

        try:
            # åŸ·è¡Œè«‹æ±‚
            result = await task

            # å¿«å–çµæœ
            self.cache.put(key, result)

            return result

        except Exception as e:
            # è«‹æ±‚å¤±æ•—ï¼Œä¸å¿«å–
            raise

        finally:
            # æ¸…ç† pending
            async with self._lock:
                if key in self.pending:
                    del self.pending[key]

    def _generate_key(
        self,
        model: str,
        content: str,
        config: Optional[Dict] = None
    ) -> str:
        """
        ç”Ÿæˆå»é‡éµ

        ä½¿ç”¨ SHA256 hash ç¢ºä¿ï¼š
        1. ç›¸åŒå…§å®¹ â†’ ç›¸åŒéµ
        2. ä¸åŒå…§å®¹ â†’ ä¸åŒéµ
        3. éµé•·åº¦å›ºå®šï¼ˆä¸å—å…§å®¹é•·åº¦å½±éŸ¿ï¼‰

        Args:
            model: æ¨¡å‹åç¨±
            content: è«‹æ±‚å…§å®¹
            config: é…ç½®åƒæ•¸ï¼ˆå¯é¸ï¼‰

        Returns:
            å»é‡éµï¼ˆSHA256 hexï¼‰
        """
        # æ¨™æº–åŒ–å…§å®¹ï¼ˆç§»é™¤å¤šé¤˜ç©ºç™½ã€çµ±ä¸€æ›è¡Œï¼‰
        normalized = re.sub(r'\s+', ' ', content.strip())

        # æ§‹å»ºéµå­—ä¸²
        key_parts = [model, normalized]

        # åŠ å…¥é…ç½®åƒæ•¸ï¼ˆå¦‚æœæœ‰ï¼‰
        if config:
            # åªè€ƒæ…®å½±éŸ¿çµæœçš„åƒæ•¸
            relevant_params = ['temperature', 'top_p', 'top_k', 'max_tokens']
            for param in relevant_params:
                if param in config:
                    key_parts.append(f"{param}={config[param]}")

        key_string = "|".join(key_parts)

        # ç”Ÿæˆ hash
        return hashlib.sha256(key_string.encode()).hexdigest()

    def get_stats(self) -> Dict[str, Any]:
        """
        ç²å–çµ±è¨ˆè³‡è¨Š

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        total = self.stats['total_requests']
        cache_hits = self.stats['cache_hits']
        dedup_hits = self.stats['dedup_hits']

        return {
            **self.stats,
            'hit_rate': (
                (cache_hits + dedup_hits) / total
            ) if total > 0 else 0,
            'cache_hit_rate': cache_hits / total if total > 0 else 0,
            'dedup_hit_rate': dedup_hits / total if total > 0 else 0,
            'cache_size': self.cache.size()
        }

    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.cache.clear()

    async def close(self):
        """é—œé–‰å»é‡å™¨ï¼ˆæ¸…ç†è³‡æºï¼‰"""
        # ç­‰å¾…æ‰€æœ‰æ­£åœ¨åŸ·è¡Œçš„è«‹æ±‚å®Œæˆ
        if self.pending:
            await asyncio.gather(*self.pending.values(), return_exceptions=True)

        self.pending.clear()
        self.cache.clear()


# ==================== ä½¿ç”¨ç¯„ä¾‹ ====================

if __name__ == "__main__":
    import random

    async def demo():
        """ç¤ºç¯„è«‹æ±‚å»é‡"""
        print("\n" + "="*70)
        print("è«‹æ±‚å»é‡å™¨ç¤ºç¯„")
        print("="*70 + "\n")

        # æ¨¡æ“¬ API èª¿ç”¨
        api_call_count = 0

        async def mock_fetch(request_id: str) -> str:
            """æ¨¡æ“¬ API èª¿ç”¨"""
            nonlocal api_call_count
            api_call_count += 1

            await asyncio.sleep(0.1)  # æ¨¡æ“¬ç¶²è·¯å»¶é²
            return f"Result for {request_id}"

        # å‰µå»ºå»é‡å™¨
        deduper = RequestDeduplicator(
            max_cache_size=100,
            ttl=60,
            verbose=True
        )

        # æ¸¬è©¦ 1ï¼šé‡è¤‡è«‹æ±‚å»é‡
        print("æ¸¬è©¦ 1ï¼šæäº¤ 10 å€‹ç›¸åŒè«‹æ±‚")
        print("-" * 70)

        api_call_count = 0
        start = time.time()

        tasks = [
            deduper.get_or_fetch(
                model='gemini-2.5-flash',
                content='åˆ†æé€™å¼µåœ–ç‰‡',
                fetch_func=lambda: mock_fetch('same_request')
            )
            for _ in range(10)
        ]

        results = await asyncio.gather(*tasks)
        elapsed = time.time() - start

        print(f"\nâœ… å®Œæˆ {len(results)} å€‹è«‹æ±‚")
        print(f"â±ï¸  ç¸½è€—æ™‚ï¼š{elapsed:.2f}s")
        print(f"ğŸ“ API èª¿ç”¨æ¬¡æ•¸ï¼š{api_call_count} æ¬¡ï¼ˆé æœŸï¼š1 æ¬¡ï¼‰")

        stats = deduper.get_stats()
        print(f"ğŸ“Š å‘½ä¸­ç‡ï¼š{stats['hit_rate']*100:.1f}%")
        print(f"   å¿«å–å‘½ä¸­ï¼š{stats['cache_hits']}")
        print(f"   ä¸¦ç™¼å»é‡ï¼š{stats['dedup_hits']}")
        print(f"   å¿«å–æœªå‘½ä¸­ï¼š{stats['cache_misses']}")

        # æ¸¬è©¦ 2ï¼šæ··åˆè«‹æ±‚ï¼ˆéƒ¨åˆ†é‡è¤‡ï¼‰
        print("\næ¸¬è©¦ 2ï¼šæ··åˆè«‹æ±‚ï¼ˆ50% é‡è¤‡ï¼‰")
        print("-" * 70)

        api_call_count = 0
        requests = ['A', 'B', 'C'] * 10  # 30 å€‹è«‹æ±‚ï¼Œ3 å€‹å”¯ä¸€

        start = time.time()

        tasks = [
            deduper.get_or_fetch(
                model='gemini-2.5-flash',
                content=f'è«‹æ±‚ {req}',
                fetch_func=lambda r=req: mock_fetch(r)
            )
            for req in requests
        ]

        results = await asyncio.gather(*tasks)
        elapsed = time.time() - start

        print(f"\nâœ… å®Œæˆ {len(results)} å€‹è«‹æ±‚")
        print(f"â±ï¸  ç¸½è€—æ™‚ï¼š{elapsed:.2f}s")
        print(f"ğŸ“ API èª¿ç”¨æ¬¡æ•¸ï¼š{api_call_count} æ¬¡ï¼ˆé æœŸï¼š3 æ¬¡ï¼‰")

        stats = deduper.get_stats()
        print(f"ğŸ“Š ç¸½é«”å‘½ä¸­ç‡ï¼š{stats['hit_rate']*100:.1f}%")
        print(f"   å¿«å–å¤§å°ï¼š{stats['cache_size']} é …ç›®")

        print("\n" + "="*70)
        print("ç¤ºç¯„å®Œæˆ")
        print("="*70 + "\n")

        await deduper.close()

    asyncio.run(demo())
